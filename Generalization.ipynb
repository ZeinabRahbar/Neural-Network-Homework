{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145a1e64",
   "metadata": {},
   "source": [
    "# 4-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb08504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "       \n",
    "        self.w=np.random.randint(-2, 2, (n_inputs, n_neurons))\n",
    "        self.b = np.random.randint(-2, 2, (1, n_neurons))    #b\n",
    "        self.weight_history = 0\n",
    "        self.bias_history = 0\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        self.input = inputs  #p\n",
    "        self.output = np.dot(inputs,self.w)+self.b\n",
    "        #print(self.output)\n",
    "        \n",
    "    def backward(self,b_input):\n",
    "        #print(type(b_input))\n",
    "        #print(b_input)\n",
    "        #print(type(self.w))\n",
    "        #print(self.w)\n",
    "        self.b_output = np.dot(b_input,self.w.T)\n",
    "        self.g_w = np.dot(self.input.T,b_input)\n",
    "        self.g_b = np.sum(b_input,axis=0,keepdims=True)\n",
    "\n",
    "class Linear:\n",
    "    def forward(self,inputs):\n",
    "        self.input = inputs\n",
    "        self.output = inputs\n",
    "    def backward(self,b_input):\n",
    "        self.b_output = b_input  \n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,inputs):\n",
    "        self.input = inputs\n",
    "        self.output = 1/(1+np.exp(-inputs))\n",
    "    def backward(self,b_input):\n",
    "        self.b_output = b_input*self.output*(1-self.output)\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        self.input = inputs\n",
    "    \n",
    "    def backward(self,b_input):\n",
    "        self.b_output = b_input\n",
    "        self.b_output[self.input<=0] = 0\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self,learning_rate = 0.001,momentum=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "    def update(self,layer):\n",
    "        if self.momentum:\n",
    "            weight_update = self.momentum*layer.weight_history\\\n",
    "            + (1-self.momentum)*(-self.learning_rate*layer.g_w)\n",
    "            layer.weight_update = weight_update\n",
    "            bias_update = self.momentum*layer.weight_history\\\n",
    "            + (1-self.momentum)*(-self.learning_rate*layer.g_b)\n",
    "            layer.bias_update = bias_update\n",
    "        else:\n",
    "            weight_update = - self.learning_rate*layer.g_w\n",
    "            bias_update = - self.learning_rate*layer.g_b\n",
    "        \n",
    "        layer.w = layer.w + weight_update\n",
    "        layer.b = layer.b + bias_update\n",
    "    \n",
    "    def discard(self,layer):\n",
    "        layer.w = layer.w \n",
    "        layer.b = layer.b \n",
    "\n",
    "class Mean_Square_Error_loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,y_predict,y_true):\n",
    "        return np.mean((y_true-y_predict)**2,axis = 0)\n",
    "    \n",
    "    def backward(self,y_predict,y_true):\n",
    "        self.b_output = -2*(y_true-y_predict)\n",
    "\n",
    "p=[]\n",
    "t=[]\n",
    "\n",
    "p1=[]\n",
    "t1=[]\n",
    "\n",
    "p_test=[]\n",
    "t_test=[]\n",
    "\n",
    "p_val=[]\n",
    "t_val=[]\n",
    "\n",
    "for i in range(400):\n",
    "    dataP = np.random.randint(-4,4)\n",
    "    p1.append(dataP)\n",
    "    \n",
    "    epsilon=np.random.normal(0, 1)\n",
    "    \n",
    "    dataT=(16+epsilon-(dataP**2))**(1/2)\n",
    "    t1.append(dataT)\n",
    "\n",
    "\n",
    "\n",
    "p = pd.DataFrame(p1,columns =['x_train'])\n",
    "t = pd.DataFrame(t1,columns =['y_train'])\n",
    "\n",
    "for i in range(100):\n",
    "    dataP = np.random.randint(-3,3)\n",
    "    p_test.append(dataP)\n",
    "    \n",
    "    epsilon=np.random.normal(0, 1)\n",
    "    \n",
    "    dataT=(16+epsilon-(dataP**2))**(1/2)\n",
    "    t_test.append(dataT)\n",
    "        \n",
    "p_test=pd.DataFrame(p_test,columns =['x_test'])\n",
    "t_test=pd.DataFrame(t_test,columns =['y_test'])\n",
    "\n",
    "\n",
    "#Creating validation data\n",
    "for i in range(100):\n",
    "    dataP = np.random.randint(0,399)\n",
    "    p_val.append(p1[dataP])\n",
    "    t_val.append(t1[dataP])\n",
    "    \n",
    "p_val=pd.DataFrame(p_val,columns =['x_val'])\n",
    "t_val=pd.DataFrame(t_val,columns =['y_val'])\n",
    "\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "#Creating train data\n",
    "for i in range(400):\n",
    "    temp=[]\n",
    "    temp.append(p['x_train'][i])\n",
    "    x_train.append(temp)\n",
    "    \n",
    "for i in range(400):\n",
    "    temp=[]\n",
    "    temp.append(t['y_train'][i])\n",
    "    y_train.append(temp)\n",
    "\n",
    "#Creating test data\n",
    "for i in range(100):\n",
    "    temp=[]\n",
    "    temp.append(p_test['x_test'][i])\n",
    "    x_test.append(temp)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    temp=[]\n",
    "    temp.append(t_test['y_test'][i])\n",
    "    y_test.append(temp)\n",
    "    \n",
    "#Creating validation data\n",
    "for i in range(100):\n",
    "    temp=[]\n",
    "    temp.append(p_val['x_val'][i])\n",
    "    x_val.append(temp)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    temp=[]\n",
    "    temp.append(t_val['y_val'][i])\n",
    "    y_val.append(temp)\n",
    "    \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0387567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss for training data: [33.72955364+0.41566225j]\n",
      "Loss for validation data: [272.83684449+61.7548134j]\n",
      "--------------------------\n",
      "Epoch: 1\n",
      "Loss for training data: [12693.14479864+20504.90561768j]\n",
      "Loss for validation data: [-3.25567443e+13+2.34583122e+12j]\n",
      "--------------------------\n",
      "Epoch: 2\n",
      "Loss for training data: [-3.25579359e+38+7.13660583e+37j]\n",
      "Loss for validation data: [-7.38929492e+115+5.58657475e+115j]\n",
      "--------------------------\n",
      "Epoch: 3\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 4\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 5\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 6\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 7\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 8\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 9\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 10\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 11\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 12\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 13\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 14\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 15\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 16\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 17\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 18\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n",
      "Epoch: 19\n",
      "Loss for training data: [nan+nanj]\n",
      "Loss for validation data: [nan+nanj]\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\3685962073.py:59: DeprecationWarning: The Python built-in `round` is deprecated for complex scalars, and will raise a `TypeError` in a future release. Use `np.round` or `scalar.round` instead.\n",
      "  loss_valid[0]=round(loss_valid[0],12)\n",
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\3091186822.py:82: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean((y_true-y_predict)**2,axis = 0)\n",
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\3091186822.py:82: RuntimeWarning: invalid value encountered in square\n",
      "  return np.mean((y_true-y_predict)**2,axis = 0)\n",
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\3091186822.py:53: RuntimeWarning: invalid value encountered in less_equal\n",
      "  self.b_output[self.input<=0] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for testing dataset: [nan+nanj]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:136: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2_1_a neural network model\n",
    "Layer1 = Dense(1,5)\n",
    "Act1 = ReLU()\n",
    "Layer2 = Dense(5,1)\n",
    "Act2 = Linear()\n",
    "Loss = Mean_Square_Error_loss()\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "\n",
    "c= 0\n",
    "y_predict = 0\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    \n",
    "    Layer1.forward(x_train)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_train)\n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_train == y_predict)\n",
    "    print(\"Epoch:\",epoch,)\n",
    "    print(\"Loss for training data:\",loss)\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_train)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    \n",
    "    \n",
    "    Layer1.forward(x_val)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_val)\n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_val == y_predict)\n",
    "    print(\"Loss for validation data:\",loss)\n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_val)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    loss_valid = Loss.forward(Act2.output,y_val)\n",
    "    loss_valid[0]=round(loss_valid[0],12)\n",
    "    preLossValid=loss_valid\n",
    "\n",
    "    if epoch==0: \n",
    "        pass\n",
    "    else:\n",
    "        if loss_valid[0] >= preLossValid[0] :\n",
    "            \n",
    "            if c<2:\n",
    "                c+=1\n",
    "            else:\n",
    "                break \n",
    "        else:\n",
    "            c=0\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "\n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output)\n",
    "Act2.forward(Layer2.output)\n",
    "\n",
    "a = Act2.output\n",
    "\n",
    "plt.scatter(p,a, c='blue')\n",
    "\n",
    "loss = Loss.forward(Act2.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abad3cd6",
   "metadata": {},
   "source": [
    "Loss for test data is equal to loss in epoch 20 for training and validation data and lower than epoch1 loss. More epochs lead to less loss. And after updating params, loss for test data would be less.\n",
    "Using early stopping lead us to have an expectation in loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588fa74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss for training data: [7.49847124-0.00055639j]\n",
      "Loss for validation data: [6.19302803-0.00899821j]\n",
      "--------------------------\n",
      "Epoch: 1\n",
      "Loss for training data: [6.20473203-0.07035186j]\n",
      "Loss for validation data: [5.92800209-0.01962394j]\n",
      "--------------------------\n",
      "Epoch: 2\n",
      "Loss for training data: [6.04313088-0.0869547j]\n",
      "Loss for validation data: [5.83282514-0.01599289j]\n",
      "--------------------------\n",
      "Epoch: 3\n",
      "Loss for training data: [5.95404549-0.08610639j]\n",
      "Loss for validation data: [5.77187368-0.01407533j]\n",
      "--------------------------\n",
      "Loss for testing dataset: [6.70841784-0.00497917j]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1867785691.py:59: DeprecationWarning: The Python built-in `round` is deprecated for complex scalars, and will raise a `TypeError` in a future release. Use `np.round` or `scalar.round` instead.\n",
      "  loss_valid[0]=round(loss_valid[0],12)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2_1_b neural network model\n",
    "Layer1 = Dense(1,5)\n",
    "Act1 = ReLU()\n",
    "Layer2 = Dense(5,1)\n",
    "Act2 = Sigmoid()\n",
    "Loss = Mean_Square_Error_loss()\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "\n",
    "c= 0\n",
    "y_predict = 0\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    \n",
    "    Layer1.forward(x_train)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_train)\n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_train == y_predict)\n",
    "    print(\"Epoch:\",epoch,)\n",
    "    print(\"Loss for training data:\",loss)\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_train)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    \n",
    "    \n",
    "    Layer1.forward(x_val)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_val)\n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_val == y_predict)\n",
    "    print(\"Loss for validation data:\",loss)\n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_val)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    loss_valid = Loss.forward(Act2.output,y_val)\n",
    "    loss_valid[0]=round(loss_valid[0],12)\n",
    "    preLossValid=loss_valid\n",
    "\n",
    "    if epoch==0: \n",
    "        pass\n",
    "    else:\n",
    "        if loss_valid[0] >= preLossValid[0] :\n",
    "            \n",
    "            if c<2:\n",
    "                c+=1\n",
    "            else:\n",
    "                break \n",
    "        else:\n",
    "            c=0\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "\n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output)\n",
    "Act2.forward(Layer2.output)\n",
    "\n",
    "a = Act2.output\n",
    "\n",
    "plt.scatter(p,a, c='blue')\n",
    "\n",
    "loss = Loss.forward(Act2.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b055f96",
   "metadata": {},
   "source": [
    "Loss for test data is equal to loss is decreased. More epochs lead to less loss. And after updating params, loss for test data would be less.\n",
    "Using early stopping lead us to have an expectation in loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8ce0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1 = Dense(1,10)\n",
    "\n",
    "Act1 = ReLU()\n",
    "\n",
    "Layer2 = Dense(10,5)\n",
    "\n",
    "Act2 =  ReLU()\n",
    "\n",
    "Layer3 = Dense(5,1)\n",
    "\n",
    "Act3 = Linear()\n",
    "\n",
    "Loss = Mean_Square_Error_loss()\n",
    "\n",
    "Optimizer = SGD(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d96414",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Epoch:0\n",
      "Loss: [47.9996137+0.74819205j]\n",
      "Validation data\n",
      "Loss: [10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81221604-8.46926605e-02j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:1\n",
      "Loss: [12.67181868-0.55157065j]\n",
      "Validation data\n",
      "Loss: [11.76736874-5.14647121e-02j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:2\n",
      "Loss: [7.41466772-0.0559653j]\n",
      "Validation data\n",
      "Loss: [10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:3\n",
      "Loss: [5.12893956-0.10067681j]\n",
      "Validation data\n",
      "Loss: [10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:4\n",
      "Loss: [3.6748044-0.13519661j]\n",
      "Validation data\n",
      "Loss: [10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:5\n",
      "Loss: [2.7511426-0.16201284j]\n",
      "Validation data\n",
      "Loss: [10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:6\n",
      "Loss: [2.1655868-0.18295407j]\n",
      "Validation data\n",
      "Loss: [10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:7\n",
      "Loss: [1.7953013-0.19937952j]\n",
      "Validation data\n",
      "Loss: [10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j 10.81184155+3.43132961e-19j\n",
      " 10.81184155+3.43132961e-19j]\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1790819274.py:33: DeprecationWarning: The Python built-in `round` is deprecated for complex scalars, and will raise a `TypeError` in a future release. Use `np.round` or `scalar.round` instead.\n",
      "  item=round(item,12)\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for epoch in range(20):\n",
    "    \n",
    "    #forward\n",
    "    Layer1.forward(x_train)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss_train = Loss.forward(Act3.output,y_train)\n",
    "\n",
    "    #backward\n",
    "    Loss.backward(Act3.output,y_train)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    Act2.backward(Layer3.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    Layer1.forward(x_val)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    \n",
    "    loss_valid = Loss.forward(Act2.output,y_val)\n",
    "    \n",
    "    for item in loss_valid:\n",
    "        \n",
    "        item=round(item,12)\n",
    "    if epoch==0: \n",
    "        pass\n",
    "    else:\n",
    "        if loss_valid[0] >= preLossValid[0] :\n",
    "            \n",
    "            if c<5:\n",
    "                c+=1\n",
    "            else:\n",
    "                break \n",
    "        else:\n",
    "            c=0\n",
    "    print('Train data')\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss_train}')\n",
    "\n",
    "    print('Validation data')\n",
    "    print(f'Loss: {loss_valid}')\n",
    "    print('----------------------------')\n",
    "\n",
    "    Loss.backward(Act3.output,y_val)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    Act2.backward(Layer3.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    preLossValid=loss_valid\n",
    "    #update params\n",
    "\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cabdd40",
   "metadata": {},
   "source": [
    "Loss for test data is equal to loss is decreased. More epochs lead to less loss. And after updating params, loss for test data would be less.\n",
    "Using early stopping lead us to have an expectation in loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da571b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for testing dataset: [1.21795992-0.00831542j]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeT0lEQVR4nO3dfWyV9f3/8ddpKacVT4929fSGHrWCNxiiGFYZoHwhdgWXKM02o8tmrfNma06JHd7CRAY6m6jLvB2oca2LY3VxKximTKzSSmxxwxGBzUqLtSCeimjPKY0W1nN+f5Adf7U39LTlnHcPz0dyEs91Ptd1PuficPXpua4eHOFwOCwAAADDkuI9AQAAgOMhWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzCBYAAGDehHhPYCyEQiEdOHBALpdLDocj3tMBAADDEA6H1dXVpdzcXCUlDf0ZSkIEy4EDB+T1euM9DQAAMAL79u1TXl7ekGMSIlhcLpekYy84PT09zrMBAADDEQwG5fV6Iz/Hh5IQwfK/00Dp6ekECwAA48xwLufgolsAAGAewQIAAMwjWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzCBYAAGAewQIAAMwjWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzCBYAAGAewQIAAMwjWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzCBYAAGAewQIAAMwjWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzogqWyspKFRQUyOVyyePxqLi4WM3Nzcddr7OzUz6fTzk5OXI6nTrvvPP0yiuv9Bnz1FNP6eyzz1ZqaqpmzZqld955J7pXAgAAElZUwVJfXy+fz6empiZt3rxZR48eVVFRkbq7uwdd58iRI/rud7+rtrY2vfTSS2pubtazzz6ryZMnR8a8+OKLWrp0qVauXKl3331XF198sRYuXKhPP/105K8MAAAkDEc4HA6PdOWDBw/K4/Govr5e8+bNG3DM2rVr9fDDD+v9999XSkrKgGNmzZqlgoICPfnkk5KkUCgkr9erJUuW6J577jnuPILBoNxutwKBgNLT00f6cgAAQAxF8/N7VNewBAIBSVJGRsagY15++WXNnj1bPp9PWVlZmj59uh588EH19vZKOvYJzPbt21VYWPj1pJKSVFhYqMbGxgG32dPTo2Aw2OcGAAAS14iDJRQKqaKiQnPnztX06dMHHbd371699NJL6u3t1SuvvKIVK1boN7/5jR544AFJ0meffabe3l5lZWX1WS8rK0t+v3/AbVZWVsrtdkduXq93pC8DAACMAyMOFp/Pp127dqmmpmbIcaFQSB6PR88884xmzpypa6+9Vr/85S+1du3akT61li1bpkAgELnt27dvxNsCAAD2TRjJSuXl5dq4caMaGhqUl5c35NicnBylpKQoOTk5smzatGny+/06cuSIMjMzlZycrI6Ojj7rdXR0KDs7e8BtOp1OOZ3OkUwdAACMQ1F9whIOh1VeXq7a2lq98cYbys/PP+46c+fOVUtLi0KhUGTZBx98oJycHE2cOFETJ07UzJkzVVdXF3k8FAqprq5Os2fPjmZ6AAAgQUUVLD6fTy+88ILWrVsnl8slv98vv9+vL7/8MjKmpKREy5Yti9wvKyvT559/rttuu00ffPCB/va3v+nBBx+Uz+eLjFm6dKmeffZZPf/88/rPf/6jsrIydXd368YbbxyDlwgAAMa7qE4JrVmzRpI0f/78PsurqqpUWloqSWpvb1dS0tcd5PV69fe//12/+MUvdNFFF2ny5Mm67bbbdPfdd0fGXHvttTp48KDuu+8++f1+zZgxQ5s2bep3IS4AADg5jep7WKzge1gAABh/YvY9LAAAALFAsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMC8qIKlsrJSBQUFcrlc8ng8Ki4uVnNz85DrVFdXy+Fw9Lmlpqb2GVNaWtpvzKJFi6J/NQAAICFNiGZwfX29fD6fCgoK9N///lfLly9XUVGR/v3vf2vSpEmDrpeent4nbBwOR78xixYtUlVVVeS+0+mMZmoAACCBRRUsmzZt6nO/urpaHo9H27dv17x58wZdz+FwKDs7e8htO53O444BAAAnp1FdwxIIBCRJGRkZQ447fPiwzjrrLHm9Xi1evFi7d+/uN2bLli3yeDw6//zzVVZWpkOHDg26vZ6eHgWDwT43AACQuBzhcDg8khVDoZCuvvpqdXZ2auvWrYOOa2xs1J49e3TRRRcpEAjokUceUUNDg3bv3q28vDxJUk1NjU455RTl5+ertbVVy5cv16mnnqrGxkYlJyf32+avfvUrrVq1qt/yQCCg9PT0kbwcAAAQY8FgUG63e1g/v0ccLGVlZXr11Ve1devWSHgMx9GjRzVt2jT96Ec/0v333z/gmL1792rKlCl6/fXXdcUVV/R7vKenRz09PZH7wWBQXq+XYAEAYByJJlhGdEqovLxcGzdu1JtvvhlVrEhSSkqKLrnkErW0tAw65pxzzlFmZuagY5xOp9LT0/vcAABA4ooqWMLhsMrLy1VbW6s33nhD+fn5UT9hb2+vdu7cqZycnEHH7N+/X4cOHRpyDAAAOHlEFSw+n08vvPCC1q1bJ5fLJb/fL7/fry+//DIypqSkRMuWLYvcX716tV577TXt3btX7777rn7yk5/oo48+0s033yzp2AW5d955p5qamtTW1qa6ujotXrxYU6dO1cKFC8foZQIAgPEsql9rXrNmjSRp/vz5fZZXVVWptLRUktTe3q6kpK876IsvvtAtt9wiv9+v008/XTNnztTbb7+tCy+8UJKUnJys9957T88//7w6OzuVm5uroqIi3X///XwXCwAAkDSKi24tieaiHQAAYMMJv+gWAAAglggWAABgHsECAADMI1gAAIB5BAsAADCPYAEAAOYRLAAAwDyCBQAAmEewAAAA8wgWAABgHsECAADMI1gAAIB5BAsAADCPYAEAAOYRLAAAwDyCBQAAmEewAAAA8wgWAABgHsECAADMI1gAAIB5BAsAADCPYAEAAOYRLAAAwDyCBQAAmEewAAAA8wgWAABgHsECAADMI1gAAIB5BAsAADCPYAEAAOYRLAAAwDyCBQAAmEewAAAA8wgWAABgHsECAADMI1gAAIB5BAsAADCPYAEAAOYRLAAAwDyCBQAAmEewAAAA8wgWAABgHsECAADMI1gAAIB5BAsAADCPYAEAAOYRLAAAwDyCBQAAmEewAAAA8wgWAABgHsECAADMI1gAAIB5BAsAADCPYAEAAOYRLAAAwDyCBQAAmEewAAAA86IKlsrKShUUFMjlcsnj8ai4uFjNzc1DrlNdXS2Hw9Hnlpqa2mdMOBzWfffdp5ycHKWlpamwsFB79uyJ/tUAAICEFFWw1NfXy+fzqampSZs3b9bRo0dVVFSk7u7uIddLT0/XJ598Erl99NFHfR5/6KGH9Pjjj2vt2rXatm2bJk2apIULF+qrr76K/hUBAICEMyGawZs2bepzv7q6Wh6PR9u3b9e8efMGXc/hcCg7O3vAx8LhsB599FHde++9Wrx4sSTpD3/4g7KysrR+/Xpdd9110UwRAAAkoFFdwxIIBCRJGRkZQ447fPiwzjrrLHm9Xi1evFi7d++OPPbhhx/K7/ersLAwssztdmvWrFlqbGwccHs9PT0KBoN9bgAAIHGNOFhCoZAqKio0d+5cTZ8+fdBx559/vn7/+99rw4YNeuGFFxQKhTRnzhzt379fkuT3+yVJWVlZfdbLysqKPPZNlZWVcrvdkZvX6x3pywAAAOPAiIPF5/Np165dqqmpGXLc7NmzVVJSohkzZuj//u//9Ne//lVnnHGGnn766ZE+tZYtW6ZAIBC57du3b8TbAgAA9kV1Dcv/lJeXa+PGjWpoaFBeXl5U66akpOiSSy5RS0uLJEWubeno6FBOTk5kXEdHh2bMmDHgNpxOp5xO50imDgAAxqGoPmEJh8MqLy9XbW2t3njjDeXn50f9hL29vdq5c2ckTvLz85Wdna26urrImGAwqG3btmn27NlRbx8AACSeqD5h8fl8WrdunTZs2CCXyxW5xsTtdistLU2SVFJSosmTJ6uyslKStHr1an3nO9/R1KlT1dnZqYcfflgfffSRbr75ZknHfoOooqJCDzzwgM4991zl5+drxYoVys3NVXFx8Ri+VAAAMF5F9QnLmjVrFAgENH/+fOXk5ERuL774YmRMe3u7Pvnkk8j9L774QrfccoumTZum733vewoGg3r77bd14YUXRsbcddddWrJkiW699VYVFBTo8OHD2rRpU78vmIu1+nrJ4fj6Vl8f1+kkrJYWaeLEY/t44sRj9zH2tm/v+37evj3eM0pc//63lJx8bD8nJx+7j7HHsSM2Nm3qe+z4xjecxIwjHA6H4/PUYycYDMrtdisQCCg9PX1MtulwDP7Y+N9jdiQlDbw/HQ4pFIr9fBIV7+fYYV/HBseO2DjR7+dofn7zbwkNYKg/oOE8juEZ7IAjHVuexLtzTPB+jh32dWxw7IgNa+9n/li/YbinfTg9NDotLcev83CYj3hHa7infTg9NHrDPe3D6aHR4dgRG8M97RPL00OcEvqGaIpx/O+5+Jk4UTp69PjjUlKkI0dO/HwSFe/n2ElOHt6piKQkqbf3xM8nUXHsiI1YHTs4JQTzhnPAiWYcEG/DvW6C6ytGh2PHyYtgQVykpIztOCDehnvdBNdXjA7HjpMXf3W+YcuWsR2HgXG+Pzb++c+xHYfB7dw5tuMwMI4dsfHqq2M7bixwDcsAhnPubvzvtfgb6kp/iV9PHCu8n2OHfR0bHDtiIxbvZ65hGaXhXIGO0QuFBv8LwQFn7PB+jh32dWxw7IgNa+9ngmUQ4XD/0z5btnDAGWuhkLRnz9fnm1NSjt3ngDO2wuH+p33++U/ezydCOCzt3v31tSpJScfus6/HFseO2AiH+5/2efXV+LyfOSUEAADiglNCAAAgoRAsAADAPIIFAACYR7AAAADzCBYAAGAewQIAAMwjWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzCBYAAGAewQIAAMwjWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzCBYAAGAewQIAAMwjWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzCBYAAGAewQIAAMwjWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzCBYAAGAewQIAAMwjWAAAgHkECwAAMI9gAQAA5hEsAADAPIIFAACYR7AAAADzCBYAAGAewQIAAMyLKlgqKytVUFAgl8slj8ej4uJiNTc3D3v9mpoaORwOFRcX91leWloqh8PR57Zo0aJopgYAABJYVMFSX18vn8+npqYmbd68WUePHlVRUZG6u7uPu25bW5vuuOMOXX755QM+vmjRIn3yySeR25/+9KdopgYAABLYhGgGb9q0qc/96upqeTwebd++XfPmzRt0vd7eXv34xz/WqlWr9NZbb6mzs7PfGKfTqezs7GimAwAAThKjuoYlEAhIkjIyMoYct3r1ank8Ht10002DjtmyZYs8Ho/OP/98lZWV6dChQ4OO7enpUTAY7HMDAACJK6pPWP5/oVBIFRUVmjt3rqZPnz7ouK1bt+q5557Tjh07Bh2zaNEiff/731d+fr5aW1u1fPlyXXnllWpsbFRycnK/8ZWVlVq1atVIpw4AAMYZRzgcDo9kxbKyMr366qvaunWr8vLyBhzT1dWliy66SL/73e905ZVXSjp2gW1nZ6fWr18/6Lb37t2rKVOm6PXXX9cVV1zR7/Genh719PRE7geDQXm9XgUCAaWnp4/k5QAAgBgLBoNyu93D+vk9ok9YysvLtXHjRjU0NAwaK5LU2tqqtrY2XXXVVZFloVDo2BNPmKDm5mZNmTKl33rnnHOOMjMz1dLSMmCwOJ1OOZ3OkUwdAACMQ1EFSzgc1pIlS1RbW6stW7YoPz9/yPEXXHCBdu7c2WfZvffeq66uLj322GPyer0Drrd//34dOnRIOTk50UwPAAAkqKiCxefzad26ddqwYYNcLpf8fr8kye12Ky0tTZJUUlKiyZMnq7KyUqmpqf2ubznttNMkKbL88OHDWrVqlX7wgx8oOztbra2tuuuuuzR16lQtXLhwtK8PAAAkgKiCZc2aNZKk+fPn91leVVWl0tJSSVJ7e7uSkob/y0fJycl677339Pzzz6uzs1O5ubkqKirS/fffz2kfAAAgaRQX3VoSzUU7AADAhmh+fvNvCQEAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJgXVbBUVlaqoKBALpdLHo9HxcXFam5uHvb6NTU1cjgcKi4u7rM8HA7rvvvuU05OjtLS0lRYWKg9e/ZEMzUAAJDAogqW+vp6+Xw+NTU1afPmzTp69KiKiorU3d193HXb2tp0xx136PLLL+/32EMPPaTHH39ca9eu1bZt2zRp0iQtXLhQX331VTTTAwAACcoRDofDI1354MGD8ng8qq+v17x58wYd19vbq3nz5umnP/2p3nrrLXV2dmr9+vWSjn26kpubq9tvv1133HGHJCkQCCgrK0vV1dW67rrrjjuPYDAot9utQCCg9PT0kb4cAAAQQ9H8/B7VNSyBQECSlJGRMeS41atXy+Px6Kabbur32Icffii/36/CwsLIMrfbrVmzZqmxsXE00wMAAAliwkhXDIVCqqio0Ny5czV9+vRBx23dulXPPfecduzYMeDjfr9fkpSVldVneVZWVuSxb+rp6VFPT0/kfjAYjHL2AABgPBnxJyw+n0+7du1STU3NoGO6urp0/fXX69lnn1VmZuZIn6qfyspKud3uyM3r9Y7ZtgEAgD0j+oSlvLxcGzduVENDg/Ly8gYd19raqra2Nl111VWRZaFQ6NgTT5ig5uZmZWdnS5I6OjqUk5MTGdfR0aEZM2YMuN1ly5Zp6dKlkfvBYJBoAQAggUUVLOFwWEuWLFFtba22bNmi/Pz8IcdfcMEF2rlzZ59l9957r7q6uvTYY4/J6/UqJSVF2dnZqquriwRKMBjUtm3bVFZWNuB2nU6nnE5nNFMHAADjWFTB4vP5tG7dOm3YsEEulytyjYnb7VZaWpokqaSkRJMnT1ZlZaVSU1P7Xd9y2mmnSVKf5RUVFXrggQd07rnnKj8/XytWrFBubm6/72sBAAAnp6iCZc2aNZKk+fPn91leVVWl0tJSSVJ7e7uSkqK7NOauu+5Sd3e3br31VnV2duqyyy7Tpk2blJqaGtV2AABAYhrV97BYwfewAAAw/sTse1gAAABigWABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMmxHsCYyEcDkuSgsFgnGcCAACG638/t//3c3woCREsXV1dkiSv1xvnmQAAgGh1dXXJ7XYPOcYRHk7WGBcKhXTgwAG5XC45HI4x3XYwGJTX69W+ffuUnp4+ptvG19jPscF+jh32dWywn2PjRO3ncDisrq4u5ebmKilp6KtUEuITlqSkJOXl5Z3Q50hPT+cvQwywn2OD/Rw77OvYYD/HxonYz8f7ZOV/uOgWAACYR7AAAADzCJbjcDqdWrlypZxOZ7ynktDYz7HBfo4d9nVssJ9jw8J+ToiLbgEAQGLjExYAAGAewQIAAMwjWAAAgHkECwAAMI9gicLVV1+tM888U6mpqcrJydH111+vAwcOxHtaCaWtrU033XST8vPzlZaWpilTpmjlypU6cuRIvKeWkH79619rzpw5OuWUU3TaaafFezoJ46mnntLZZ5+t1NRUzZo1S++88068p5RwGhoadNVVVyk3N1cOh0Pr16+P95QSUmVlpQoKCuRyueTxeFRcXKzm5ua4zIVgicKCBQv05z//Wc3NzfrLX/6i1tZW/fCHP4z3tBLK+++/r1AopKefflq7d+/Wb3/7W61du1bLly+P99QS0pEjR3TNNdeorKws3lNJGC+++KKWLl2qlStX6t1339XFF1+shQsX6tNPP4331BJKd3e3Lr74Yj311FPxnkpCq6+vl8/nU1NTkzZv3qyjR4+qqKhI3d3dMZ8Lv9Y8Ci+//LKKi4vV09OjlJSUeE8nYT388MNas2aN9u7dG++pJKzq6mpVVFSos7Mz3lMZ92bNmqWCggI9+eSTko79W2der1dLlizRPffcE+fZJSaHw6Ha2loVFxfHeyoJ7+DBg/J4PKqvr9e8efNi+tx8wjJCn3/+uf74xz9qzpw5xMoJFggElJGREe9pAMd15MgRbd++XYWFhZFlSUlJKiwsVGNjYxxnBoyNQCAgSXE5JhMsUbr77rs1adIkfetb31J7e7s2bNgQ7ykltJaWFj3xxBP62c9+Fu+pAMf12Wefqbe3V1lZWX2WZ2Vlye/3x2lWwNgIhUKqqKjQ3LlzNX369Jg//0kfLPfcc48cDseQt/fffz8y/s4779S//vUvvfbaa0pOTlZJSYk4q3Z80e5nSfr444+1aNEiXXPNNbrlllviNPPxZyT7GgCOx+fzadeuXaqpqYnL80+Iy7Macvvtt6u0tHTIMeecc07kvzMzM5WZmanzzjtP06ZNk9frVVNTk2bPnn2CZzq+RbufDxw4oAULFmjOnDl65plnTvDsEku0+xpjJzMzU8nJyero6OizvKOjQ9nZ2XGaFTB65eXl2rhxoxoaGpSXlxeXOZz0wXLGGWfojDPOGNG6oVBIktTT0zOWU0pI0eznjz/+WAsWLNDMmTNVVVWlpKST/oPAqIzmPY3RmThxombOnKm6urrIBaChUEh1dXUqLy+P7+SAEQiHw1qyZIlqa2u1ZcsW5efnx20uJ32wDNe2bdv0j3/8Q5dddplOP/10tba2asWKFZoyZQqfroyhjz/+WPPnz9dZZ52lRx55RAcPHow8xv+hjr329nZ9/vnnam9vV29vr3bs2CFJmjp1qk499dT4Tm6cWrp0qW644QZ9+9vf1qWXXqpHH31U3d3duvHGG+M9tYRy+PBhtbS0RO5/+OGH2rFjhzIyMnTmmWfGcWaJxefzad26ddqwYYNcLlfkWiy32620tLTYTiaMYXnvvffCCxYsCGdkZISdTmf47LPPDv/85z8P79+/P95TSyhVVVVhSQPeMPZuuOGGAff1m2++Ge+pjWtPPPFE+MwzzwxPnDgxfOmll4abmpriPaWE8+abbw743r3hhhviPbWEMtjxuKqqKuZz4XtYAACAeVwcAAAAzCNYAACAeQQLAAAwj2ABAADmESwAAMA8ggUAAJhHsAAAAPMIFgAAYB7BAgAAzCNYAACAeQQLAAAwj2ABAADm/T+IFIn2c0bScAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output)\n",
    "Act2.forward(Layer2.output)\n",
    "\n",
    "Layer3.forward(Act2.output)\n",
    "Act3.forward(Layer3.output)\n",
    "\n",
    "a = Act3.output\n",
    "\n",
    "plt.scatter(p,a, c='blue')\n",
    "\n",
    "loss = Loss.forward(Act3.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53996a4d",
   "metadata": {},
   "source": [
    "# Loss for test data is equal to loss in epoch 20 for training data and lower than epoch1 loss. More epochs lead to less loss. And after updating params, loss for test data would be less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e3b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2_1_C\n",
    "Layer1 = Dense(1,10)\n",
    "\n",
    "Act1 = Sigmoid()\n",
    "\n",
    "Layer2 = Dense(10,5)\n",
    "\n",
    "Act2 = Sigmoid()\n",
    "\n",
    "Layer3 = Dense(5,1)\n",
    "\n",
    "Act3 = Linear()\n",
    "\n",
    "Loss = Mean_Square_Error_loss()\n",
    "\n",
    "Optimizer = SGD(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd83d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Epoch:0\n",
      "Loss: [6.41344681-0.09036677j]\n",
      "Validation data\n",
      "Loss: [ 9.45672628-2.11300586e-04j 10.72607981-5.73307687e-05j\n",
      " 10.77722571-9.46049408e-05j 10.53504134-9.80626602e-04j\n",
      " 10.8035238 -9.89424268e-06j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:1\n",
      "Loss: [4.30308039-0.1336808j]\n",
      "Validation data\n",
      "Loss: [ 9.65356899-1.60767875e-04j 10.72731596-5.86492171e-05j\n",
      " 10.7770562 -9.70108598e-05j 10.51825381-1.16216741e-03j\n",
      " 10.80362361-9.75801383e-06j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:2\n",
      "Loss: [3.08240507-0.16566726j]\n",
      "Validation data\n",
      "Loss: [ 9.77508897-2.04503921e-04j 10.72791373-6.04776171e-05j\n",
      " 10.77690186-9.93804370e-05j 10.503896  -1.35303969e-03j\n",
      " 10.80368816-9.73399271e-06j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:3\n",
      "Loss: [2.35693988-0.18972091j]\n",
      "Validation data\n",
      "Loss: [ 9.85468184-2.80335151e-04j 10.7281585 -6.22120131e-05j\n",
      " 10.77677441-1.01592596e-04j 10.49215769-1.54315482e-03j\n",
      " 10.8037309 -9.76010473e-06j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:4\n",
      "Loss: [1.92016891-0.20799389j]\n",
      "Validation data\n",
      "Loss: [ 9.90955144-3.67746882e-04j 10.72821251-6.36525393e-05j\n",
      " 10.77667606-1.03601026e-04j 10.48305972-1.72420413e-03j\n",
      " 10.80376014-9.80592240e-06j]\n",
      "----------------------------\n",
      "Train data\n",
      "Epoch:5\n",
      "Loss: [1.65562611-0.22196531j]\n",
      "Validation data\n",
      "Loss: [ 9.9489901 -4.59443209e-04j 10.72816423-6.47623093e-05j\n",
      " 10.77660476-1.05397890e-04j 10.47645194-1.89026520e-03j\n",
      " 10.80378081-9.85689993e-06j]\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1790819274.py:33: DeprecationWarning: The Python built-in `round` is deprecated for complex scalars, and will raise a `TypeError` in a future release. Use `np.round` or `scalar.round` instead.\n",
      "  item=round(item,12)\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for epoch in range(20):\n",
    "    \n",
    "    #forward\n",
    "    Layer1.forward(x_train)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss_train = Loss.forward(Act3.output,y_train)\n",
    "\n",
    "    #backward\n",
    "    Loss.backward(Act3.output,y_train)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    Act2.backward(Layer3.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    Layer1.forward(x_val)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    \n",
    "    loss_valid = Loss.forward(Act2.output,y_val)\n",
    "    \n",
    "    for item in loss_valid:\n",
    "        \n",
    "        item=round(item,12)\n",
    "    if epoch==0: \n",
    "        pass\n",
    "    else:\n",
    "        if loss_valid[0] >= preLossValid[0] :\n",
    "            \n",
    "            if c<5:\n",
    "                c+=1\n",
    "            else:\n",
    "                break \n",
    "        else:\n",
    "            c=0\n",
    "    print('Train data')\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss_train}')\n",
    "\n",
    "    print('Validation data')\n",
    "    print(f'Loss: {loss_valid}')\n",
    "    print('----------------------------')\n",
    "\n",
    "    Loss.backward(Act3.output,y_val)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    Act2.backward(Layer3.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    preLossValid=loss_valid\n",
    "    #update params\n",
    "\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69de9607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for testing dataset: [0.99281886-0.00663069j]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output)\n",
    "Act2.forward(Layer2.output)\n",
    "\n",
    "Layer3.forward(Act2.output)\n",
    "Act3.forward(Layer3.output)\n",
    "\n",
    "a = Act3.output\n",
    "\n",
    "plt.scatter(p,a, c='blue')\n",
    "\n",
    "loss = Loss.forward(Act3.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4f980",
   "metadata": {},
   "source": [
    "Loss for test data is equal to loss is decreased. More epochs lead to less loss. And after updating params, loss for test data would be less.\n",
    "Using early stopping lead us to have an expectation in loss decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8f0b9",
   "metadata": {},
   "source": [
    "# 4-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc99b8",
   "metadata": {},
   "source": [
    "# a , dropout rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ebcea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_d:\n",
    "\n",
    "    def __init__(self,n_inputs,n_neurons,Pd):\n",
    "        self.Pdropout=Pd\n",
    "        self.w=np.random.randint(-2, 2, (n_inputs, n_neurons))\n",
    "        self.b = np.random.randint(-2, 2, (1, n_neurons))    #b\n",
    "        self.weight_history = 0\n",
    "        self.bias_history = 0\n",
    "\n",
    "    def forward(self,inputs,t):\n",
    "        self.input = inputs  #p\n",
    "        if t!='test':\n",
    "            for i in range(len(self.w)):\n",
    "                for j in range(len(self.w[i])):\n",
    "                    p=np.random.uniform()\n",
    "                    if p>=0 and p<self.Pdropout:\n",
    "                        self.w[i][j]=0\n",
    "        self.output = np.dot(inputs,self.w)+self.b\n",
    "\n",
    "    def backward(self,b_input,t):\n",
    "        if t!='test':\n",
    "            for i in range(len(self.w)):\n",
    "                for j in range(len(self.w[i])):\n",
    "                    p=np.random.uniform()\n",
    "                    if p>=0 and p<self.Pdropout:\n",
    "                        self.w[i][j]=0\n",
    "        \n",
    "        self.b_output = np.dot(b_input,self.w.T)\n",
    "        self.g_w = np.dot(self.input.T,b_input)\n",
    "        self.g_b = np.sum(b_input,axis=0,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd15b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: [49.59340661+1.08072184j]\n",
      "--------------------------\n",
      "Epoch: 1\n",
      "Loss: [679.23584793+14.89569077j]\n",
      "--------------------------\n",
      "Epoch: 2\n",
      "Loss: [338.88623808+8.60310375j]\n",
      "--------------------------\n",
      "Epoch: 3\n",
      "Loss: [14.69318808+0.096163j]\n",
      "--------------------------\n",
      "Epoch: 4\n",
      "Loss: [1.72546609-0.24411463j]\n",
      "--------------------------\n",
      "Epoch: 5\n",
      "Loss: [1.20675721-0.25772573j]\n",
      "--------------------------\n",
      "Epoch: 6\n",
      "Loss: [1.18600885-0.25827018j]\n",
      "--------------------------\n",
      "Epoch: 7\n",
      "Loss: [1.18517892-0.25829195j]\n",
      "--------------------------\n",
      "Epoch: 8\n",
      "Loss: [1.18514572-0.25829283j]\n",
      "--------------------------\n",
      "Epoch: 9\n",
      "Loss: [1.18514439-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 10\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 11\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 12\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 13\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 14\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 15\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 16\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 17\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 18\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch: 19\n",
      "Loss: [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Loss for testing dataset: [0.37055704-0.03160329j]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 2_1_a neural network model\n",
    "Layer1 = Dense_d(1,5,0.2)\n",
    "Act1 = ReLU()\n",
    "Layer2 = Dense(5,1)\n",
    "Act2 = Linear()\n",
    "Loss = Mean_Square_Error_loss()\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "\n",
    "\n",
    "y_predict = 0\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    \n",
    "    Layer1.forward(x_train,0.2)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_train)\n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_train == y_predict)\n",
    "    print(\"Epoch:\",epoch,)\n",
    "    print(\"Loss:\",loss)\n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_train)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,0.2)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "\n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p,0.2)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output)\n",
    "Act2.forward(Layer2.output)\n",
    "\n",
    "a = Act2.output\n",
    "\n",
    "plt.scatter(p,a)\n",
    "\n",
    "loss = Loss.forward(Act2.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6b2b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: [34.21928238+0.2591555j]\n",
      "--------------------------\n",
      "Epoch: 1\n",
      "Loss: [33.12699669+0.35635629j]\n",
      "--------------------------\n",
      "Epoch: 2\n",
      "Loss: [25.40277889+0.19934837j]\n",
      "--------------------------\n",
      "Epoch: 3\n",
      "Loss: [8.32404076-0.28347847j]\n",
      "--------------------------\n",
      "Epoch: 4\n",
      "Loss: [1.67541391-0.23511813j]\n",
      "--------------------------\n",
      "Epoch: 5\n",
      "Loss: [1.32321317-0.27792137j]\n",
      "--------------------------\n",
      "Epoch: 6\n",
      "Loss: [1.22344474-0.26690147j]\n",
      "--------------------------\n",
      "Epoch: 7\n",
      "Loss: [1.07953357-0.23212041j]\n",
      "--------------------------\n",
      "Epoch: 8\n",
      "Loss: [1.00481602-0.21876312j]\n",
      "--------------------------\n",
      "Epoch: 9\n",
      "Loss: [0.9729133-0.21454533j]\n",
      "--------------------------\n",
      "Epoch: 10\n",
      "Loss: [0.95984732-0.2111254j]\n",
      "--------------------------\n",
      "Epoch: 11\n",
      "Loss: [0.94142022-0.20362534j]\n",
      "--------------------------\n",
      "Epoch: 12\n",
      "Loss: [1.1759921-0.25286868j]\n",
      "--------------------------\n",
      "Epoch: 13\n",
      "Loss: [1.15018011-0.24618912j]\n",
      "--------------------------\n",
      "Epoch: 14\n",
      "Loss: [1.02165023-0.21356534j]\n",
      "--------------------------\n",
      "Epoch: 15\n",
      "Loss: [1.12131068-0.2352812j]\n",
      "--------------------------\n",
      "Epoch: 16\n",
      "Loss: [1.00580888-0.2018869j]\n",
      "--------------------------\n",
      "Epoch: 17\n",
      "Loss: [0.96263899-0.19962744j]\n",
      "--------------------------\n",
      "Epoch: 18\n",
      "Loss: [1.12101093-0.22899322j]\n",
      "--------------------------\n",
      "Epoch: 19\n",
      "Loss: [1.02930027-0.21170525j]\n",
      "--------------------------\n",
      "Loss for testing dataset: [0.31186978-0.03320587j]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 2_1_b neural network model\n",
    "Layer1 = Dense_d(1,5,0.2)\n",
    "Act1 = Sigmoid()\n",
    "Layer2 = Dense(5,1)\n",
    "Act2 = Linear()\n",
    "Loss = Mean_Square_Error_loss()\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "\n",
    "\n",
    "y_predict = 0\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    \n",
    "    Layer1.forward(x_train,0.2)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_train)\n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_train == y_predict)\n",
    "    print(\"Epoch:\",epoch,)\n",
    "    print(\"Loss:\",loss)\n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_train)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,0.2)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "\n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p,0.2)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output)\n",
    "Act2.forward(Layer2.output)\n",
    "\n",
    "a = Act2.output\n",
    "\n",
    "plt.scatter(p,a)\n",
    "\n",
    "loss = Loss.forward(Act2.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "531a310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "Loss [10.83689109+5.12440949e-18j]\n",
      "--------------------------\n",
      "Epoch 1 :\n",
      "Loss [1.57121421-0.24796115j]\n",
      "--------------------------\n",
      "Epoch 2 :\n",
      "Loss [2.57596436-0.26221449j]\n",
      "--------------------------\n",
      "Epoch 3 :\n",
      "Loss [5.23011127-0.26230559j]\n",
      "--------------------------\n",
      "Epoch 4 :\n",
      "Loss [5.80846426-0.26658689j]\n",
      "--------------------------\n",
      "Epoch 5 :\n",
      "Loss [2.57502045-0.2592651j]\n",
      "--------------------------\n",
      "Epoch 6 :\n",
      "Loss [1.24073938-0.25833175j]\n",
      "--------------------------\n",
      "Epoch 7 :\n",
      "Loss [1.18736814-0.25829442j]\n",
      "--------------------------\n",
      "Epoch 8 :\n",
      "Loss [1.18523329-0.25829292j]\n",
      "--------------------------\n",
      "Epoch 9 :\n",
      "Loss [1.18514789-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 10 :\n",
      "Loss [1.18514448-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 11 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 12 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 13 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 14 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 15 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 16 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 17 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 18 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 19 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Loss for testing dataset: [0.37055704-0.03160329j]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2_1_c neural network model\n",
    "\n",
    "Layer1 = Dense_d(1,10,0.2)\n",
    "Act1 = ReLU()\n",
    "\n",
    "Layer2 = Dense_d(10,5,0.2)\n",
    "Act2 = ReLU()\n",
    "\n",
    "Layer3 = Dense(5,1)\n",
    "Act3 = Linear()\n",
    "\n",
    "Loss = Mean_Square_Error_loss()\n",
    "\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    Layer1.forward(x_train,0.2)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output,0.2)\n",
    "    Act2.forward(Layer2.output)\n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss = Loss.forward(Act3.output,y_train)\n",
    "    \n",
    "    print(\"Epoch\", epoch,\":\")\n",
    "    print(\"Loss\", loss)    \n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act3.output,y_train)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    Act2.backward(Layer3.b_output)\n",
    "    \n",
    "    Layer2.backward(Act2.b_output,0.2)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,0.2)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)\n",
    "    \n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p,0.2)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output,0.2)\n",
    "Act2.forward(Layer2.output)\n",
    "Layer3.forward(Act2.output)\n",
    "Act3.forward(Layer3.output)\n",
    "a = Act3.output\n",
    "\n",
    "plt.scatter(p,a)\n",
    "\n",
    "loss = Loss.forward(Act3.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "631754db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "Loss [32.85180957+0.22814791j]\n",
      "--------------------------\n",
      "Epoch 1 :\n",
      "Loss [1.53107218-0.24976905j]\n",
      "--------------------------\n",
      "Epoch 2 :\n",
      "Loss [1.15381973-0.25123063j]\n",
      "--------------------------\n",
      "Epoch 3 :\n",
      "Loss [1.06845172-0.24232507j]\n",
      "--------------------------\n",
      "Epoch 4 :\n",
      "Loss [1.23293301-0.23425753j]\n",
      "--------------------------\n",
      "Epoch 5 :\n",
      "Loss [1.27561905-0.25691465j]\n",
      "--------------------------\n",
      "Epoch 6 :\n",
      "Loss [1.19357587-0.25690293j]\n",
      "--------------------------\n",
      "Epoch 7 :\n",
      "Loss [1.18629438-0.25784221j]\n",
      "--------------------------\n",
      "Epoch 8 :\n",
      "Loss [1.18647086-0.25782817j]\n",
      "--------------------------\n",
      "Epoch 9 :\n",
      "Loss [1.18558688-0.25826848j]\n",
      "--------------------------\n",
      "Epoch 10 :\n",
      "Loss [1.18575986-0.25805035j]\n",
      "--------------------------\n",
      "Epoch 11 :\n",
      "Loss [1.18554697-0.25829907j]\n",
      "--------------------------\n",
      "Epoch 12 :\n",
      "Loss [1.18534653-0.25827563j]\n",
      "--------------------------\n",
      "Epoch 13 :\n",
      "Loss [1.1851529-0.25827222j]\n",
      "--------------------------\n",
      "Epoch 14 :\n",
      "Loss [1.18520351-0.25829446j]\n",
      "--------------------------\n",
      "Epoch 15 :\n",
      "Loss [1.18517569-0.25829508j]\n",
      "--------------------------\n",
      "Epoch 16 :\n",
      "Loss [1.18515883-0.25829437j]\n",
      "--------------------------\n",
      "Epoch 17 :\n",
      "Loss [1.18514934-0.25829342j]\n",
      "--------------------------\n",
      "Epoch 18 :\n",
      "Loss [1.1851472-0.25829369j]\n",
      "--------------------------\n",
      "Epoch 19 :\n",
      "Loss [1.18514523-0.25829315j]\n",
      "--------------------------\n",
      "Loss for testing dataset: [0.37192248-0.03189822j]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2_1_d neural network model\n",
    "\n",
    "Layer1 = Dense_d(1,10,0.2)\n",
    "Act1 = Sigmoid()\n",
    "\n",
    "Layer2 = Dense_d(10,5,0.2)\n",
    "Act2 = Sigmoid()\n",
    "\n",
    "Layer3 = Dense(5,1)\n",
    "Act3 = Linear()\n",
    "\n",
    "Loss = Mean_Square_Error_loss()\n",
    "\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    Layer1.forward(x_train,0.2)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output,0.2)\n",
    "    Act2.forward(Layer2.output)\n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss = Loss.forward(Act3.output,y_train)\n",
    "    \n",
    "    print(\"Epoch\", epoch,\":\")\n",
    "    print(\"Loss\", loss)    \n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act3.output,y_train)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    Act2.backward(Layer3.b_output)\n",
    "    \n",
    "    Layer2.backward(Act2.b_output,0.2)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,0.2)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)\n",
    "    \n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p,0.2)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output,0.2)\n",
    "Act2.forward(Layer2.output)\n",
    "Layer3.forward(Act2.output)\n",
    "Act3.forward(Layer3.output)\n",
    "a = Act3.output\n",
    "\n",
    "plt.scatter(p,a)\n",
    "\n",
    "loss = Loss.forward(Act3.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319e647",
   "metadata": {},
   "source": [
    "# b dropout rate = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6bb1dc",
   "metadata": {},
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c9cdf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: [23.4864379+0.08313245j]\n",
      "--------------------------\n",
      "Epoch: 1\n",
      "Loss: [474.17998916+74.39864196j]\n",
      "--------------------------\n",
      "Epoch: 2\n",
      "Loss: [1.7681642e+08+91541688.32867962j]\n",
      "--------------------------\n",
      "Epoch: 3\n",
      "Loss: [2.07489497e+24+1.50066343e+25j]\n",
      "--------------------------\n",
      "Epoch: 4\n",
      "Loss: [4.68631601e+23+3.39005758e+24j]\n",
      "--------------------------\n",
      "Epoch: 5\n",
      "Loss: [1.8745264e+22+1.35602303e+23j]\n",
      "--------------------------\n",
      "Epoch: 6\n",
      "Loss: [7.49810562e+20+5.42409213e+21j]\n",
      "--------------------------\n",
      "Epoch: 7\n",
      "Loss: [2.99924225e+19+2.16963685e+20j]\n",
      "--------------------------\n",
      "Epoch: 8\n",
      "Loss: [1.1996969e+18+8.6785474e+18j]\n",
      "--------------------------\n",
      "Epoch: 9\n",
      "Loss: [4.7987876e+16+3.47141896e+17j]\n",
      "--------------------------\n",
      "Epoch: 10\n",
      "Loss: [1.91951504e+15+1.38856758e+16j]\n",
      "--------------------------\n",
      "Epoch: 11\n",
      "Loss: [7.67806015e+13+5.55427034e+14j]\n",
      "--------------------------\n",
      "Epoch: 12\n",
      "Loss: [3.07122406e+12+2.22170814e+13j]\n",
      "--------------------------\n",
      "Epoch: 13\n",
      "Loss: [1.22848962e+11+8.88683254e+11j]\n",
      "--------------------------\n",
      "Epoch: 14\n",
      "Loss: [4.9139585e+09+3.55473302e+10j]\n",
      "--------------------------\n",
      "Epoch: 15\n",
      "Loss: [1.96558341e+08+1.42189321e+09j]\n",
      "--------------------------\n",
      "Epoch: 16\n",
      "Loss: [7862334.78206197+56875728.00132023j]\n",
      "--------------------------\n",
      "Epoch: 17\n",
      "Loss: [314494.52902104+2275028.87209166j]\n",
      "--------------------------\n",
      "Epoch: 18\n",
      "Loss: [12580.9188994+91000.90692252j]\n",
      "--------------------------\n",
      "Epoch: 19\n",
      "Loss: [504.37449454+3639.78831575j]\n",
      "--------------------------\n",
      "Loss for testing dataset: [28.11019395+150.86567257j]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGdCAYAAADaPpOnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAemklEQVR4nO3df3TT9b3H8VdKNdTahNAGWo6FUplW1KO1k26sd7fMHpTdq6c7CFOORZCjwFFUwEkRXItnCq7b9Ew9Ov+wunP1oByscmB3B2eZYuWHUstEbY4CFSwU0V4TKBhakvuHM1eubWggaXhnz8c5Ocekn2/yzteYPE2/fHGEw+GwAAAAjEhL9gAAAACxIF4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgSnqyB4i3UCikffv2KSsrSw6HI9njAACAfgiHwzp06JBGjBihtLTo362kXLzs27dP+fn5yR4DAACcgr179+q8886Luibl4iUrK0vSN0/e5XIleRoAANAfgUBA+fn5kc/xaFIuXr79VZHL5SJeAAAwpj+HfHDALgAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAExJaLwUFBTI4XCccFmxYkXUbTo6OlRVVaXc3FxlZmbqiiuu0OrVqxM5JgAAMCQ90Q/wwAMP6NZbb41cz8rKirp++vTp+uqrr7RmzRrl5OTohRde0NSpU/Xuu++quLg40eMCAIAzXMJ/bZSVlaXc3NzIJTMzM+r6t99+W/PmzdO4ceNUWFiopUuXasiQIdq2bVuiRwUAAAYkPF5WrFih7OxsFRcXq66uTj09PVHXjx8/Xi+++KI6OzsVCoW0cuVKff311yovL+91fTAYVCAQOOECAABSV0J/bXTnnXfqiiuu0NChQ/X2229r8eLF2r9/v/7whz/0uc1LL72kX/7yl8rOzlZ6errOOeccNTQ0aMyYMb2uX758uZYtW5aopwAAAM4wjnA4HI5lg+rqaj388MNR13z00UcqKir63u3PPPOMZs+ercOHD8vpdPa67bx587R161Y99NBDysnJ0SuvvKJHHnlEGzdu1KWXXvq99cFgUMFgMHI9EAgoPz9ffr9fLpcrlqcGAACSJBAIyO129+vzO+Z4OXjwoL788suoawoLC3X22Wd/7/YPPvhAl1xyiVpbW3XhhRd+7+c7d+7UmDFjtGPHDl188cWR2ysqKjRmzBg99dRTJ50vlicPAADODLF8fsf8ayOv1yuv13tKg7W0tCgtLU3Dhg3r9edHjhyRJKWlnXgozqBBgxQKhU7pMQEAQGpJ2AG7mzZt0qOPPqrt27dr165dev755zV//nzddNNN8ng8kqT29nYVFRVp69atkqSioiKNGTNGs2fP1tatW7Vz5079/ve/12uvvabKyspEjQoAAAxJ2AG7TqdTK1euVG1trYLBoEaPHq358+drwYIFkTXd3d3y+XyRb1zOOuss/eUvf1F1dbWuvfZaHT58WGPGjNFzzz2nn//854kaFQAAGBLzMS9nOo55AQDAnlg+v/m7jQAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYQrwAAABTiBcAAGAK8QIAAEwhXgAAgCnECwAAMIV4AQAAphAvAADAFOIFAACYktB4KSgokMPhOOGyYsWKqNvs3LlTv/jFL+T1euVyuTR16lQdOHAgkWMCAABDEv7NywMPPKD9+/dHLvPmzetzbVdXlyZOnCiHw6HGxkY1NTXp2LFjuvbaaxUKhRI9KgAAMCA90Q+QlZWl3Nzcfq1tampSW1ub3nvvPblcLknSc889J4/Ho8bGRlVUVCRyVAAAYEDCv3lZsWKFsrOzVVxcrLq6OvX09PS5NhgMyuFwyOl0Rm4bPHiw0tLS9NZbb/W5TSAQOOECAABSV0Lj5c4779TKlSu1YcMGzZ49Ww899JDuvffePtf/6Ec/UmZmphYtWqQjR46oq6tL99xzj44fP679+/f3us3y5cvldrsjl/z8/EQ9HQAAcAZwhMPhcCwbVFdX6+GHH4665qOPPlJRUdH3bn/mmWc0e/ZsHT58+IRvV75r/fr1mjt3rnbv3q20tDTdeOON+vDDDzVu3Dg9+eST31sfDAYVDAYj1wOBgPLz8+X3+yO/egIAAGe2QCAgt9vdr8/vmI95WbhwoWbMmBF1TWFhYa+3l5aWqqenR21tbbrwwgt7XTNx4kTt3LlTX3zxhdLT0zVkyBDl5ub2eZ9Op7PPEAIAAKkn5njxer3yer2n9GAtLS1KS0vTsGHDTro2JydHktTY2KjPP/9c11133Sk9JgAASC0J+9NGmzZt0pYtWzRhwgRlZWVp06ZNmj9/vm666SZ5PB5JUnt7u6666ir9+c9/1rhx4yRJ9fX1uuiii+T1erVp0ybdddddmj9/fp/f1AAAgH8tCYsXp9OplStXqra2VsFgUKNHj9b8+fO1YMGCyJru7m75fD4dOXIkcpvP59PixYvV2dmpgoICLVmyRPPnz0/UmAAAwJiYD9g908VywA8AADgzxPL5zd9tBAAATCFeAACAKcQLAAAwhXgBAACmEC8AAMAU4gUAAJhCvAAAAFOIFwAAYArxAgAATCFeAACAKcQLAAAwhXgBAACmEC8AAMAU4gUAAJhCvAAAAFOIFwAAYArxAgAATCFeAACAKcQLAAAwhXgBAACmEC8AAMAU4gUAAJhCvAAAAFOIFwAAYArxAgAATCFeAACAKcQLAAAwhXgBAACmEC8AAMAU4gUAAJhCvAAAAFOIFwAAYArxAgAATCFeAACAKcQLAAAwhXgBAACmEC8AAMAU4gUAAJhCvAAAAFOIFwAAYArxAgAATCFeAACAKcQLAAAwhXgBAACmJDxe1q1bp9LSUmVkZMjj8aiysjLq+nA4rF//+tfKy8tTRkaGKioq9PHHHyd6TAAAYERC42X16tWqqqrSzJkztX37djU1NWnatGlRt/ntb3+rP/7xj3rqqae0ZcsWZWZm6uqrr9bXX3+dyFEBAIARjnA4HE7EHff09KigoEDLli3TrFmz+rVNOBzWiBEjtHDhQt1zzz2SJL/fr+HDh+vZZ5/VDTfccNL7CAQCcrvd8vv9crlcp/UcAADAwIjl8zth37w0Nzervb1daWlpKi4uVl5eniZNmqQdO3b0uc3u3bvV0dGhioqKyG1ut1ulpaXatGlTokYFAACGJCxedu3aJUmqra3V0qVLtXbtWnk8HpWXl6uzs7PXbTo6OiRJw4cPP+H24cOHR372/wWDQQUCgRMuAAAgdcUcL9XV1XI4HFEvra2tCoVCkqQlS5Zo8uTJKikpUX19vRwOh1atWhW3J7B8+XK53e7IJT8/P273DQAAzjzpsW6wcOFCzZgxI+qawsJC7d+/X5I0duzYyO1Op1OFhYXas2dPr9vl5uZKkg4cOKC8vLzI7QcOHNDll1/e6zaLFy/WggULItcDgQABAwBACos5Xrxer7xe70nXlZSUyOl0yufzqaysTJLU3d2ttrY2jRo1qtdtRo8erdzcXL3++uuRWAkEAtqyZYvmzp3b6zZOp1NOpzPWpwEAAIxK2DEvLpdLc+bMUU1NjdavXy+fzxcJkClTpkTWFRUVqaGhQZLkcDh099136ze/+Y3WrFmj999/X9OnT9eIESNOen4YAADwryHmb15iUVdXp/T0dFVVVeno0aMqLS1VY2OjPB5PZI3P55Pf749cv/fee9XV1aXbbrtNX331lcrKyvTXv/5VgwcPTuSoAADAiISd5yVZOM8LAAD2nBHneQEAAEgE4gUAAJhCvAAAAFOIFwAAYArxAgAATCFeAACAKcQLAAAwJaEnqUslb314UDf9eWvk+n9NH6eysSf/axIQm086DmvSH99Qd0g6K0367zv/XWNyz032WCln267/0eSn345cX33beJUUeqJsgVP14WcB/efjGxXSN/+3uPaOf9PY8zgHVbzx3jEwGls6dMvKbZHrz9xQop9dnjvgc3CSun4oqF7X58/aVvxHXB4D0ujqdertxeiQtJv9HDe8ngcO+3pg8N4xMBL9euYkdXEU7V9Wf36O/unrzUeSwv/8OU4fr+eBw74eGLx3DIwz7fVMvETx1ocH47oOvfuk43Cfbz7fCv9zHU7dtl3/E9d16NuHnwXiug69471jYDS2dMR1XTwQL1F89xiXeKxD7yb98Y24rkPvvnuMSzzWoW//+fjGuK5D73jvGBjfPcYlHuvigXhB0nWH4rsOSLb+vlR5SZ8e3jv+dREvSLqz+vkq7O86INn6+1LlJX16eO/418W/0ij+a/q4uK5D7/77zn+P6zr0bvVt4+O6Dn1be8e/xXUdesd7x8B45oaSuK6LB+Iliv6ex4XzvZyeMbnnynGSNY5/rsOp6+95XDjfy+nr73lcON/L6eG9Y2D09zwuA3m+F+LlJE72Z9c5V0N87F7xH32+CXGuhvjh9Txw2NcDg/eOgXGmvZ45SV0/cYbdgcFZMgcGZ9gdOJxhd2Dw3jEwEnmG3Vg+v4kXAACQdJxhFwAApCziBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmJLQeFm3bp1KS0uVkZEhj8ejysrKqOtffvllTZw4UdnZ2XI4HGppaUnkeAAAwKCExcvq1atVVVWlmTNnavv27WpqatK0adOibtPV1aWysjI9/PDDiRoLAAAYl56IO+3p6dFdd92luro6zZo1K3L72LFjo25XVVUlSWpra0vEWAAAIAUk5JuX5uZmtbe3Ky0tTcXFxcrLy9OkSZO0Y8eOuD9WMBhUIBA44QIAAFJXQuJl165dkqTa2lotXbpUa9eulcfjUXl5uTo7O+P6WMuXL5fb7Y5c8vPz43r/AADgzBJTvFRXV8vhcES9tLa2KhQKSZKWLFmiyZMnq6SkRPX19XI4HFq1alVcn8DixYvl9/sjl71798b1/gEAwJklpmNeFi5cqBkzZkRdU1hYqP3790s68RgXp9OpwsJC7dmzJ/Ypo3A6nXI6nXG9TwAAcOaKKV68Xq+8Xu9J15WUlMjpdMrn86msrEyS1N3drba2No0aNerUJgUAAFCCjnlxuVyaM2eOampqtH79evl8Ps2dO1eSNGXKlMi6oqIiNTQ0RK53dnaqpaVFH374oSTJ5/OppaVFHR0diRgTAAAYlJA/Ki1JdXV1Sk9PV1VVlY4eParS0lI1NjbK4/FE1vh8Pvn9/sj1NWvWaObMmZHrN9xwgySppqZGtbW1iRoVAAAY4giHw+FkDxFPgUBAbrdbfr9fLpcr2eMAAIB+iOXzm7/bCAAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApCY+XdevWqbS0VBkZGfJ4PKqsrOxzbXd3txYtWqRLL71UmZmZGjFihKZPn659+/YlekwAAGBEQuNl9erVqqqq0syZM7V9+3Y1NTVp2rRpfa4/cuSImpubdf/996u5uVkvv/yyfD6frrvuukSOCQAADHGEw+FwIu64p6dHBQUFWrZsmWbNmnXK9/POO+9o3Lhx+vTTTzVy5MiTrg8EAnK73fL7/XK5XKf8uAAAYODE8vmdsG9empub1d7errS0NBUXFysvL0+TJk3Sjh07Yrofv98vh8OhIUOG9PrzYDCoQCBwwgUAAKSuhMXLrl27JEm1tbVaunSp1q5dK4/Ho/LycnV2dvbrPr7++mstWrRIN954Y58Vtnz5crnd7sglPz8/bs8BAACceWKOl+rqajkcjqiX1tZWhUIhSdKSJUs0efJklZSUqL6+Xg6HQ6tWrTrp43R3d2vq1KkKh8N68skn+1y3ePFi+f3+yGXv3r2xPiUAAGBIeqwbLFy4UDNmzIi6prCwUPv375ckjR07NnK70+lUYWGh9uzZE3X7b8Pl008/VWNjY9TffTmdTjmdzv4/AQAAYFrM8eL1euX1ek+6rqSkRE6nUz6fT2VlZZK+iZK2tjaNGjWqz+2+DZePP/5YGzZsUHZ2dqwjAgCAFJawY15cLpfmzJmjmpoarV+/Xj6fT3PnzpUkTZkyJbKuqKhIDQ0Nkr4Jl+uvv17vvvuunn/+eR0/flwdHR3q6OjQsWPHEjUqAAAwJOZvXmJRV1en9PR0VVVV6ejRoyotLVVjY6M8Hk9kjc/nk9/vlyS1t7drzZo1kqTLL7/8hPvasGGDysvLEzkuAAAwIGHneUkWzvMCAIA9Z8R5XgAAABKBeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmJLweFm3bp1KS0uVkZEhj8ejysrKqOtra2tVVFSkzMxMeTweVVRUaMuWLYkeEwAAGJHQeFm9erWqqqo0c+ZMbd++XU1NTZo2bVrUbS644AI9/vjjev/99/XWW2+poKBAEydO1MGDBxM5KgAAMMIRDofDibjjnp4eFRQUaNmyZZo1a9Yp308gEJDb7dbf/vY3XXXVVf1e7/f75XK5TvlxAQDAwInl8zth37w0Nzervb1daWlpKi4uVl5eniZNmqQdO3b0+z6OHTump59+Wm63W5dddlmiRgUAAIYkLF527dol6ZtjWJYuXaq1a9fK4/GovLxcnZ2dUbddu3atzj33XA0ePFiPPPKIXnvtNeXk5PS6NhgMKhAInHABAACpK+Z4qa6ulsPhiHppbW1VKBSSJC1ZskSTJ09WSUmJ6uvr5XA4tGrVqqiPMWHCBLW0tOjtt9/WNddco6lTp+rzzz/vde3y5cvldrsjl/z8/FifEgAAMCTmY14OHjyoL7/8MuqawsJCNTU16Wc/+5k2btyosrKyyM9KS0tVUVGhBx98sN+P+YMf/EC33HKLFi9e/L2fBYNBBYPByPVAIKD8/HyOeQEAwJBYjnlJj/XOvV6vvF7vSdeVlJTI6XTK5/NF4qW7u1ttbW0aNWpUTI8ZCoVOCJTvcjqdcjqdMd0fAACwK2HHvLhcLs2ZM0c1NTVav369fD6f5s6dK0maMmVKZF1RUZEaGhokSV1dXbrvvvu0efNmffrpp9q2bZtuueUWtbe3n7ANAAD41xXzNy+xqKurU3p6uqqqqnT06FGVlpaqsbFRHo8nssbn88nv90uSBg0apNbWVj333HP64osvlJ2drSuvvFIbN27UxRdfnMhRAQCAEQk7z0uycJ4XAADsOSPO8wIAAJAIxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmJKe7AHiLRwOS5ICgUCSJwEAAP317ef2t5/j0aRcvBw6dEiSlJ+fn+RJAABArA4dOiS32x11jSPcn8QxJBQKad++fcrKypLD4YjrfQcCAeXn52vv3r1yuVxxvW/8H/bzwGA/Dxz29cBgPw+MRO3ncDisQ4cOacSIEUpLi35US8p985KWlqbzzjsvoY/hcrn4D2MAsJ8HBvt54LCvBwb7eWAkYj+f7BuXb3HALgAAMIV4AQAAphAvMXA6naqpqZHT6Uz2KCmN/Tww2M8Dh309MNjPA+NM2M8pd8AuAABIbXzzAgAATCFeAACAKcQLAAAwhXgBAACmEC+n6LrrrtPIkSM1ePBg5eXlqaqqSvv27Uv2WCmlra1Ns2bN0ujRo5WRkaHzzz9fNTU1OnbsWLJHS0kPPvigxo8fr3POOUdDhgxJ9jgp44knnlBBQYEGDx6s0tJSbd26NdkjpZw333xT1157rUaMGCGHw6FXXnkl2SOlpOXLl+vKK69UVlaWhg0bpsrKSvl8vqTMQrycogkTJuill16Sz+fT6tWrtXPnTl1//fXJHiultLa2KhQK6U9/+pM++OADPfLII3rqqad03333JXu0lHTs2DFNmTJFc+fOTfYoKePFF1/UggULVFNTo+bmZl122WW6+uqr9fnnnyd7tJTS1dWlyy67TE888USyR0lpb7zxhm6//XZt3rxZr732mrq7uzVx4kR1dXUN+Cz8Uek4WbNmjSorKxUMBnXWWWcle5yUVVdXpyeffFK7du1K9igp69lnn9Xdd9+tr776KtmjmFdaWqorr7xSjz/+uKRv/u61/Px8zZs3T9XV1UmeLjU5HA41NDSosrIy2aOkvIMHD2rYsGF644039NOf/nRAH5tvXuKgs7NTzz//vMaPH0+4JJjf79fQoUOTPQZwUseOHdO2bdtUUVERuS0tLU0VFRXatGlTEicD4sPv90tSUt6TiZfTsGjRImVmZio7O1t79uzRq6++muyRUtonn3yixx57TLNnz072KMBJffHFFzp+/LiGDx9+wu3Dhw9XR0dHkqYC4iMUCunuu+/WT37yE11yySUD/vjEy3dUV1fL4XBEvbS2tkbW/+pXv9J7772n9evXa9CgQZo+fbr4LdzJxbqfJam9vV3XXHONpkyZoltvvTVJk9tzKvsaAE7m9ttv144dO7Ry5cqkPH56Uh71DLVw4ULNmDEj6prCwsLIP+fk5CgnJ0cXXHCBLrroIuXn52vz5s368Y9/nOBJbYt1P+/bt08TJkzQ+PHj9fTTTyd4utQS675G/OTk5GjQoEE6cODACbcfOHBAubm5SZoKOH133HGH1q5dqzfffFPnnXdeUmYgXr7D6/XK6/We0rahUEiSFAwG4zlSSoplP7e3t2vChAkqKSlRfX290tL4sjAWp/Oaxuk5++yzVVJSotdffz1y8GgoFNLrr7+uO+64I7nDAacgHA5r3rx5amho0N///neNHj06abMQL6dgy5Yteuedd1RWViaPx6OdO3fq/vvv1/nnn8+3LnHU3t6u8vJyjRo1Sr/73e908ODByM/4P9f427Nnjzo7O7Vnzx4dP35cLS0tkqQxY8bo3HPPTe5wRi1YsEA333yzfvjDH2rcuHF69NFH1dXVpZkzZyZ7tJRy+PBhffLJJ5Hru3fvVktLi4YOHaqRI0cmcbLUcvvtt+uFF17Qq6++qqysrMixW263WxkZGQM7TBgx+8c//hGeMGFCeOjQoWGn0xkuKCgIz5kzJ/zZZ58le7SUUl9fH5bU6wXxd/PNN/e6rzds2JDs0Ux77LHHwiNHjgyfffbZ4XHjxoU3b96c7JFSzoYNG3p97d58883JHi2l9PV+XF9fP+CzcJ4XAABgCgcQAAAAU4gXAABgCvECAABMIV4AAIApxAsAADCFeAEAAKYQLwAAwBTiBQAAmEK8AAAAU4gXAABgCvECAABMIV4AAIAp/wtPMtTEm/9fOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 2_1_a neural network model\n",
    "Layer1 = Dense_d(1,5,0.6)\n",
    "Act1 = ReLU()\n",
    "Layer2 = Dense(5,1)\n",
    "Act2 = Linear()\n",
    "Loss = Mean_Square_Error_loss()\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "\n",
    "\n",
    "y_predict = 0\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    \n",
    "    Layer1.forward(x_train,0.6)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_train)\n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_train == y_predict)\n",
    "    print(\"Epoch:\",epoch,)\n",
    "    print(\"Loss:\",loss)\n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_train)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,0.6)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "\n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p,0.6)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output)\n",
    "Act2.forward(Layer2.output)\n",
    "\n",
    "a = Act2.output\n",
    "\n",
    "plt.scatter(p,a)\n",
    "\n",
    "loss = Loss.forward(Act2.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cd64baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: [57.90296872+0.36779744j]\n",
      "--------------------------\n",
      "Epoch: 1\n",
      "Loss: [60.32635411+0.19456499j]\n",
      "--------------------------\n",
      "Epoch: 2\n",
      "Loss: [10.65632473-0.23087413j]\n",
      "--------------------------\n",
      "Epoch: 3\n",
      "Loss: [2.48659139-0.28256723j]\n",
      "--------------------------\n",
      "Epoch: 4\n",
      "Loss: [2.03791113-0.25387122j]\n",
      "--------------------------\n",
      "Epoch: 5\n",
      "Loss: [1.72050105-0.23751612j]\n",
      "--------------------------\n",
      "Epoch: 6\n",
      "Loss: [1.24785186-0.23739779j]\n",
      "--------------------------\n",
      "Epoch: 7\n",
      "Loss: [1.17981389-0.25531885j]\n",
      "--------------------------\n",
      "Epoch: 8\n",
      "Loss: [1.18521597-0.25826991j]\n",
      "--------------------------\n",
      "Epoch: 9\n",
      "Loss: [1.17753417-0.25635887j]\n",
      "--------------------------\n",
      "Epoch: 10\n",
      "Loss: [1.16579218-0.25249381j]\n",
      "--------------------------\n",
      "Epoch: 11\n",
      "Loss: [1.17026121-0.25352265j]\n",
      "--------------------------\n",
      "Epoch: 12\n",
      "Loss: [1.18317936-0.25774465j]\n",
      "--------------------------\n",
      "Epoch: 13\n",
      "Loss: [1.17552725-0.2554392j]\n",
      "--------------------------\n",
      "Epoch: 14\n",
      "Loss: [1.18514533-0.25829311j]\n",
      "--------------------------\n",
      "Epoch: 15\n",
      "Loss: [1.17755787-0.25597254j]\n",
      "--------------------------\n",
      "Epoch: 16\n",
      "Loss: [1.18336276-0.25782743j]\n",
      "--------------------------\n",
      "Epoch: 17\n",
      "Loss: [1.18452638-0.25830755j]\n",
      "--------------------------\n",
      "Epoch: 18\n",
      "Loss: [1.17637979-0.25573231j]\n",
      "--------------------------\n",
      "Epoch: 19\n",
      "Loss: [1.18267845-0.25751444j]\n",
      "--------------------------\n",
      "Loss for testing dataset: [0.36786319-0.03131512j]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 2_1_b neural network model\n",
    "Layer1 = Dense_d(1,5,0.6)\n",
    "Act1 = Sigmoid()\n",
    "Layer2 = Dense(5,1)\n",
    "Act2 = Linear()\n",
    "Loss = Mean_Square_Error_loss()\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "\n",
    "\n",
    "y_predict = 0\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    \n",
    "    Layer1.forward(x_train,0.6)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_train)\n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_train == y_predict)\n",
    "    print(\"Epoch:\",epoch,)\n",
    "    print(\"Loss:\",loss)\n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_train)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,0.6)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "\n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p,0.6)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output)\n",
    "Act2.forward(Layer2.output)\n",
    "\n",
    "a = Act2.output\n",
    "\n",
    "plt.scatter(p,a)\n",
    "\n",
    "loss = Loss.forward(Act2.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cebe8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "Loss [10.83689109+5.12440949e-18j]\n",
      "--------------------------\n",
      "Epoch 1 :\n",
      "Loss [1.57121421-0.24796115j]\n",
      "--------------------------\n",
      "Epoch 2 :\n",
      "Loss [1.20058713-0.25787959j]\n",
      "--------------------------\n",
      "Epoch 3 :\n",
      "Loss [1.18576205-0.25827633j]\n",
      "--------------------------\n",
      "Epoch 4 :\n",
      "Loss [1.18516904-0.2582922j]\n",
      "--------------------------\n",
      "Epoch 5 :\n",
      "Loss [1.18514532-0.25829284j]\n",
      "--------------------------\n",
      "Epoch 6 :\n",
      "Loss [1.18514437-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 7 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 8 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 9 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 10 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 11 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 12 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 13 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 14 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 15 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 16 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 17 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 18 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 19 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Loss for testing dataset: [0.37055704-0.03160329j]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkSklEQVR4nO3dfXBU5cH38d8mNhtq2JUUyAvZmIQoFjXQiUiXQlEaCCm3JS12lOnTxT6ZWhjwbgYtEqqgtc6mQK0oTsB2Kn1q07TVhrbcQoqUpHUMEVIzYhgd3mx4SQKld3ZDLBua3ecP69rVBLLJsntl/X5mzow5e52z1zmum6+7h4MlEAgEBAAAYLCEWE8AAADgcggWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMa7KtYTiAS/36/Tp09r9OjRslgssZ4OAAAYhEAgoO7ubmVmZioh4dKfocRFsJw+fVoOhyPW0wAAAENw4sQJZWVlXXJMXATL6NGjJb13wDabLcazAQAAg+H1euVwOIK/xy8lLoLl/a+BbDYbwQIAwAgzmMs5uOgWAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8cIKlqqqKhUUFMhms8lms8npdGrnzp0Djv/xj3+sWbNmacyYMRozZoyKior02muvhYy55557ZLFYQpb58+cP7WgAAEBcCitYsrKyVFlZqebmZh04cEBz5szRwoUL1dra2u/4+vp6LV68WHv37lVjY6McDofmzZunU6dOhYybP3++2tvbg8svf/nLoR8RAACIO5ZAIBAYzg5SU1O1YcMGlZWVXXZsX1+fxowZo82bN8vlckl67xOWrq4ubd++fchz8Hq9stvt8ng8stlsQ94PAACInnB+fw/5Gpa+vj7V1NSop6dHTqdzUNu8++67unjxolJTU0PW19fXa/z48Zo0aZKWLVumc+fOXXI/Pp9PXq83ZAEAAPHrqnA3OHjwoJxOpy5cuKCUlBTV1tZq8uTJg9r2wQcfVGZmpoqKioLr5s+fr6985SvKzc3V0aNHtWbNGpWUlKixsVGJiYn97sftduvRRx8Nd+oAAGCECvsrod7eXrW1tcnj8eiFF17QT37yEzU0NFw2WiorK7V+/XrV19eroKBgwHHHjh3TxIkT9fLLL+sLX/hCv2N8Pp98Pl/wZ6/XK4fDwVdCAACMIFf0K6GkpCTl5+ersLBQbrdbU6ZM0aZNmy65zcaNG1VZWak//vGPl4wVScrLy9PYsWN15MiRAcdYrdbgn1R6fwEAAPEr7K+EPszv94d82vFh69ev1+OPP666ujrdcsstl93fyZMnde7cOWVkZAx3agAAIE6EFSwVFRUqKSlRdna2uru7VV1drfr6etXV1UmSXC6XJkyYILfbLUn6wQ9+oLVr16q6ulo5OTnq6OiQJKWkpCglJUXnz5/Xo48+qkWLFik9PV1Hjx7VqlWrlJ+fr+Li4ggfKgAAGKnCCpYzZ87I5XKpvb1ddrtdBQUFqqur09y5cyVJbW1tSkj44Fumqqoq9fb26s477wzZz7p16/TII48oMTFRb7zxhn72s5+pq6tLmZmZmjdvnh577DFZrdYIHB4AAIgHw74Piwm4DwsAACNPVO7DAgAAEC0ECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMF1awVFVVqaCgQDabTTabTU6nUzt37hxw/I9//GPNmjVLY8aM0ZgxY1RUVKTXXnstZEwgENDatWuVkZGhUaNGqaioSIcPHx7a0QAAgLgUVrBkZWWpsrJSzc3NOnDggObMmaOFCxeqtbW13/H19fVavHix9u7dq8bGRjkcDs2bN0+nTp0Kjlm/fr2eeuopbdmyRU1NTbr66qtVXFysCxcuDO/IAABA3LAEAoHAcHaQmpqqDRs2qKys7LJj+/r6NGbMGG3evFkul0uBQECZmZm6//779cADD0iSPB6P0tLStG3bNt19992DmoPX65XdbpfH45HNZhvO4QAAgCgJ5/f3kK9h6evrU01NjXp6euR0Oge1zbvvvquLFy8qNTVVknT8+HF1dHSoqKgoOMZut2v69OlqbGwccD8+n09erzdkAQAA8SvsYDl48KBSUlJktVq1dOlS1dbWavLkyYPa9sEHH1RmZmYwUDo6OiRJaWlpIePS0tKCj/XH7XbLbrcHF4fDEe5hAACAESTsYJk0aZJaWlrU1NSkZcuWacmSJTp06NBlt6usrFRNTY1qa2uVnJw8pMm+r6KiQh6PJ7icOHFiWPsDAABmuyrcDZKSkpSfny9JKiws1P79+7Vp0yZt3bp1wG02btyoyspKvfzyyyooKAiuT09PlyR1dnYqIyMjuL6zs1NTp04dcH9Wq1VWqzXcqQMAgBFq2Pdh8fv98vl8Az6+fv16PfbYY9q1a5duueWWkMdyc3OVnp6uPXv2BNd5vV41NTUN+roYAAAQ/8L6hKWiokIlJSXKzs5Wd3e3qqurVV9fr7q6OkmSy+XShAkT5Ha7JUk/+MEPtHbtWlVXVysnJyd4XUpKSopSUlJksVhUXl6u73//+7ruuuuUm5urhx9+WJmZmSotLY3skQIAgBErrGA5c+aMXC6X2tvbZbfbVVBQoLq6Os2dO1eS1NbWpoSEDz60qaqqUm9vr+68886Q/axbt06PPPKIJGnVqlXq6enRvffeq66uLs2cOVO7du0a9nUuAAAgfgz7Piwm4D4sAACMPFG5DwsAAEC0ECwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMF5YwVJVVaWCggLZbDbZbDY5nU7t3LlzwPGtra1atGiRcnJyZLFY9OSTT35kzCOPPCKLxRKy3HDDDWEfCAAAiF9hBUtWVpYqKyvV3NysAwcOaM6cOVq4cKFaW1v7Hf/uu+8qLy9PlZWVSk9PH3C/N954o9rb24PLK6+8Et5RAACAuHZVOIPvuOOOkJ8ff/xxVVVVad++fbrxxhs/Mn7atGmaNm2aJGn16tUDT+Kqqy4ZNAAA4ONtyNew9PX1qaamRj09PXI6ncOaxOHDh5WZmam8vDx97WtfU1tb2yXH+3w+eb3ekAUAAMSvsIPl4MGDSklJkdVq1dKlS1VbW6vJkycPeQLTp0/Xtm3btGvXLlVVVen48eOaNWuWuru7B9zG7XbLbrcHF4fDMeTnBwAA5rMEAoFAOBv09vaqra1NHo9HL7zwgn7yk5+ooaHhstGSk5Oj8vJylZeXX3JcV1eXrr32Wj3xxBMqKyvrd4zP55PP5wv+7PV65XA45PF4ZLPZwjkcAAAQI16vV3a7fVC/v8O6hkWSkpKSlJ+fL0kqLCzU/v37tWnTJm3dunVos/2Qa665Rtdff72OHDky4Bir1Sqr1RqR5wMAAOYb9n1Y/H5/yKcdw3X+/HkdPXpUGRkZEdsnAAAY2cL6hKWiokIlJSXKzs5Wd3e3qqurVV9fr7q6OkmSy+XShAkT5Ha7Jb339dGhQ4eC/3zq1Cm1tLQoJSUl+CnNAw88oDvuuEPXXnutTp8+rXXr1ikxMVGLFy+O5HECAIARLKxgOXPmjFwul9rb22W321VQUKC6ujrNnTtXktTW1qaEhA8+tDl9+rQ+85nPBH/euHGjNm7cqNmzZ6u+vl6SdPLkSS1evFjnzp3TuHHjNHPmTO3bt0/jxo2LwOEBAIB4EPZFtyYK56IdAABghnB+f/N3CQEAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4V8V6AiZ75dBZ/Z//91rw5+ddt2rm5HExnFF8OtJxXiVPNeiiX/pEgrTzv2crPz0l1tOKO83H/leLnn01+POL985QYd6YGM4ofh066dV/bf6L/Hrv/wp3rJilyVm2WE8r7vDeER1/aunQ/61pDv7807sLNWdqetTnYQkEAoGoP2uEeb1e2e12eTwe2WyReVPIWf0/Az72TuWCiDwHpNzV/6P+XoAWScc5zxHD6zl6ONfRwXtHdFzp13M4v7/5Sqgfl/oXNJjHMTgDveFIUuDfj2P4eD1HD+c6OnjviA7TXs8Ey4e8cuhsRMehf0c6zg/4hvO+wL/HYeiaj/1vRMdhYIdOeiM6Dv3jvSM6/tTSEdFxkUCwfMh/XrMSiXHoX8lTDREdh/795zUrkRiHgf3X5r9EdBz6x3tHdPznNSuRGBcJBAti4qI/suOAWBvsS5WX9PDw3vHxRbAgJj4xyFfeYMcBsTbYlyov6eHhvePji3+lH/K869aIjkP/dv737IiOQ/9evHdGRMdhYDtWzIroOPSP947o+OndhREdFwkEy4cM9j4r3I9lePLTU2S5zBjLv8dh6AZ7nxXuxzJ8g73PCvdjGR7eO6JjsPdZieb9WMIKlqqqKhUUFMhms8lms8npdGrnzp0Djm9tbdWiRYuUk5Mji8WiJ598st9xzzzzjHJycpScnKzp06frtddie0Hr5f5sOfdSiIzjlQsGfOPhXgqRw+s5ejjX0cF7R3SY9noOK1iysrJUWVmp5uZmHThwQHPmzNHChQvV2tra7/h3331XeXl5qqysVHp6/xX2q1/9SitXrtS6dev017/+VVOmTFFxcbHOnDkT/tFE0DuVCz7ytc/zrlt5w4mw45UL9HL57OD3zZ9IkF4un80bToS9U7ngI1/7vHjvDF7PV8A7lQv00opZwTfXBEkvrZjFuY4w3jui453KBR/52uendxfG5PU87DvdpqamasOGDSorK7vkuJycHJWXl6u8vDxk/fTp0zVt2jRt3rxZkuT3++VwOHTfffdp9erVg5rDlbjTLQAAuLKicqfbvr4+1dTUqKenR06nc0j76O3tVXNzs4qKij6YUEKCioqK1NjYOOB2Pp9PXq83ZAEAAPEr7GA5ePCgUlJSZLVatXTpUtXW1mry5MlDevK///3v6uvrU1paWsj6tLQ0dXQMfPc8t9stu90eXBwOx5CeHwAAjAxhB8ukSZPU0tKipqYmLVu2TEuWLNGhQ4euxNwGVFFRIY/HE1xOnDgR1ecHAADRdVW4GyQlJSk/P1+SVFhYqP3792vTpk3aunVr2E8+duxYJSYmqrOzM2R9Z2fngBfpSpLVapXVag37+QAAwMg07Puw+P1++Xy+IW2blJSkwsJC7dmzJ2R/e/bsGfJ1MQAAIP6E9QlLRUWFSkpKlJ2dre7ublVXV6u+vl51dXWSJJfLpQkTJsjtdkt676La978u6u3t1alTp9TS0qKUlJTgpzQrV67UkiVLdMstt+jWW2/Vk08+qZ6eHn3jG9+I5HECAIARLKxgOXPmjFwul9rb22W321VQUKC6ujrNnTtXktTW1qaEhA8+tDl9+rQ+85nPBH/euHGjNm7cqNmzZ6u+vl6SdNddd+ns2bNau3atOjo6NHXqVO3atesjF+ICAICPr2Hfh8UE3IcFAICRJyr3YQEAAIgWggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxgsrWKqqqlRQUCCbzSabzSan06mdO3decpvf/OY3uuGGG5ScnKybb75ZL730Usjj99xzjywWS8gyf/788I8EAADErbCCJSsrS5WVlWpubtaBAwc0Z84cLVy4UK2trf2Of/XVV7V48WKVlZXp9ddfV2lpqUpLS/Xmm2+GjJs/f77a29uDyy9/+cuhHxEAAIg7lkAgEBjODlJTU7VhwwaVlZV95LG77rpLPT092rFjR3DdZz/7WU2dOlVbtmyR9N4nLF1dXdq+ffuQ5+D1emW32+XxeGSz2Ya8HwAAED3h/P4e8jUsfX19qqmpUU9Pj5xOZ79jGhsbVVRUFLKuuLhYjY2NIevq6+s1fvx4TZo0ScuWLdO5c+cu+dw+n09erzdkAQAA8euqcDc4ePCgnE6nLly4oJSUFNXW1mry5Mn9ju3o6FBaWlrIurS0NHV0dAR/nj9/vr7yla8oNzdXR48e1Zo1a1RSUqLGxkYlJib2u1+3261HH3003KkDAIARKuxgmTRpklpaWuTxePTCCy9oyZIlamhoGDBaLufuu+8O/vPNN9+sgoICTZw4UfX19frCF77Q7zYVFRVauXJl8Gev1yuHwzGk5wcAAOYL+yuhpKQk5efnq7CwUG63W1OmTNGmTZv6HZuenq7Ozs6QdZ2dnUpPTx9w/3l5eRo7dqyOHDky4Bir1Rr8k0rvLwAAIH4N+z4sfr9fPp+v38ecTqf27NkTsm737t0DXvMiSSdPntS5c+eUkZEx3KkBAIA4EdZXQhUVFSopKVF2dra6u7tVXV2t+vp61dXVSZJcLpcmTJggt9stSfr2t7+t2bNn64c//KEWLFigmpoaHThwQM8++6wk6fz583r00Ue1aNEipaen6+jRo1q1apXy8/NVXFwc4UMFAAAjVVjBcubMGblcLrW3t8tut6ugoEB1dXWaO3euJKmtrU0JCR98aDNjxgxVV1froYce0po1a3Tddddp+/btuummmyRJiYmJeuONN/Szn/1MXV1dyszM1Lx58/TYY4/JarVG8DABAMBINuz7sJiA+7AAADDyROU+LAAAANFCsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeGEFS1VVlQoKCmSz2WSz2eR0OrVz585LbvOb3/xGN9xwg5KTk3XzzTfrpZdeCnk8EAho7dq1ysjI0KhRo1RUVKTDhw+HfyQAACBuhRUsWVlZqqysVHNzsw4cOKA5c+Zo4cKFam1t7Xf8q6++qsWLF6usrEyvv/66SktLVVpaqjfffDM4Zv369Xrqqae0ZcsWNTU16eqrr1ZxcbEuXLgwvCMDAABxwxIIBALD2UFqaqo2bNigsrKyjzx21113qaenRzt27Aiu++xnP6upU6dqy5YtCgQCyszM1P33368HHnhAkuTxeJSWlqZt27bp7rvvHtQcvF6v7Ha7PB6PbDbbcA4HAABESTi/v4d8DUtfX59qamrU09Mjp9PZ75jGxkYVFRWFrCsuLlZjY6Mk6fjx4+ro6AgZY7fbNX369OCY/vh8Pnm93pAFAADEr7CD5eDBg0pJSZHVatXSpUtVW1uryZMn9zu2o6NDaWlpIevS0tLU0dERfPz9dQON6Y/b7Zbdbg8uDocj3MMAAAAjSNjBMmnSJLW0tKipqUnLli3TkiVLdOjQoSsxtwFVVFTI4/EElxMnTkT1+QEAQHRdFe4GSUlJys/PlyQVFhZq//792rRpk7Zu3fqRsenp6ers7AxZ19nZqfT09ODj76/LyMgIGTN16tQB52C1WmW1WsOdOgAAGKGGfR8Wv98vn8/X72NOp1N79uwJWbd79+7gNS+5ublKT08PGeP1etXU1DTgdTEAAODjJ6xPWCoqKlRSUqLs7Gx1d3erurpa9fX1qqurkyS5XC5NmDBBbrdbkvTtb39bs2fP1g9/+EMtWLBANTU1OnDggJ599llJksViUXl5ub7//e/ruuuuU25urh5++GFlZmaqtLQ0skcKAABGrLCC5cyZM3K5XGpvb5fdbldBQYHq6uo0d+5cSVJbW5sSEj740GbGjBmqrq7WQw89pDVr1ui6667T9u3bddNNNwXHrFq1Sj09Pbr33nvV1dWlmTNnateuXUpOTo7QIQIAgJFu2PdhMQH3YQEAYOSJyn1YAAAAooVgAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxwgoWt9utadOmafTo0Ro/frxKS0v19ttvX3Kbixcv6nvf+54mTpyo5ORkTZkyRbt27QoZ88gjj8hisYQsN9xwQ/hHAwAA4lJYwdLQ0KDly5dr37592r17ty5evKh58+app6dnwG0eeughbd26VU8//bQOHTqkpUuX6stf/rJef/31kHE33nij2tvbg8srr7wytCMCAABxxxIIBAJD3fjs2bMaP368Ghoa9PnPf77fMZmZmfrud7+r5cuXB9ctWrRIo0aN0vPPPy/pvU9Ytm/frpaWliHNw+v1ym63y+PxyGazDWkfAAAgusL5/T2sa1g8Ho8kKTU1dcAxPp9PycnJIetGjRr1kU9QDh8+rMzMTOXl5elrX/ua2traLrlPr9cbsgAAgPg15GDx+/0qLy/X5z73Od10000DjisuLtYTTzyhw4cPy+/3a/fu3frtb3+r9vb24Jjp06dr27Zt2rVrl6qqqnT8+HHNmjVL3d3d/e7T7XbLbrcHF4fDMdTDAAAAI8CQvxJatmyZdu7cqVdeeUVZWVkDjjt79qy++c1v6g9/+IMsFosmTpyooqIi/fSnP9U///nPfrfp6urStddeqyeeeEJlZWUfedzn88nn8wV/9nq9cjgcfCUEAMAIcsW/ElqxYoV27NihvXv3XjJWJGncuHHavn27enp69Le//U1vvfWWUlJSlJeXN+A211xzja6//nodOXKk38etVqtsNlvIAgAA4ldYwRIIBLRixQrV1tbqT3/6k3Jzcwe9bXJysiZMmKB//etfevHFF7Vw4cIBx54/f15Hjx5VRkZGONMDAABxKqxgWb58uZ5//nlVV1dr9OjR6ujoUEdHR8hXOy6XSxUVFcGfm5qa9Nvf/lbHjh3TX/7yF82fP19+v1+rVq0KjnnggQfU0NCgd955R6+++qq+/OUvKzExUYsXL47AIQIAgJHuqnAGV1VVSZJuu+22kPXPPfec7rnnHklSW1ubEhI+6KALFy7ooYce0rFjx5SSkqIvfvGL+vnPf65rrrkmOObkyZNavHixzp07p3HjxmnmzJnat2+fxo0bN7SjAgAAcWVY92ExBfdhAQBg5InafVgAAACigWABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPHCunGcqd6/lYzX643xTAAAwGC9/3t7MLeEi4tg6e7uliQ5HI4YzwQAAISru7tbdrv9kmPi4k63fr9fp0+f1ujRo2WxWCK6b6/XK4fDoRMnTnAX3SuI8xwdnOfo4VxHB+c5Oq7UeQ4EAuru7lZmZmbIX+vTn7j4hCUhIUFZWVlX9DlsNhv/MUQB5zk6OM/Rw7mODs5zdFyJ83y5T1bex0W3AADAeAQLAAAwHsFyGVarVevWrZPVao31VOIa5zk6OM/Rw7mODs5zdJhwnuPiolsAABDf+IQFAAAYj2ABAADGI1gAAIDxCBYAAGA8giUMX/rSl5Sdna3k5GRlZGTo61//uk6fPh3racWVd955R2VlZcrNzdWoUaM0ceJErVu3Tr29vbGeWlx6/PHHNWPGDH3yk5/UNddcE+vpxI1nnnlGOTk5Sk5O1vTp0/Xaa6/Fekpx589//rPuuOMOZWZmymKxaPv27bGeUlxyu92aNm2aRo8erfHjx6u0tFRvv/12TOZCsITh9ttv169//Wu9/fbbevHFF3X06FHdeeedsZ5WXHnrrbfk9/u1detWtba26kc/+pG2bNmiNWvWxHpqcam3t1df/epXtWzZslhPJW786le/0sqVK7Vu3Tr99a9/1ZQpU1RcXKwzZ87EempxpaenR1OmTNEzzzwT66nEtYaGBi1fvlz79u3T7t27dfHiRc2bN089PT1Rnwt/rHkYfv/736u0tFQ+n0+f+MQnYj2duLVhwwZVVVXp2LFjsZ5K3Nq2bZvKy8vV1dUV66mMeNOnT9e0adO0efNmSe/9XWcOh0P33XefVq9eHePZxSeLxaLa2lqVlpbGeipx7+zZsxo/frwaGhr0+c9/PqrPzScsQ/SPf/xDv/jFLzRjxgxi5QrzeDxKTU2N9TSAy+rt7VVzc7OKioqC6xISElRUVKTGxsYYzgyIDI/HI0kxeU8mWML04IMP6uqrr9anPvUptbW16Xe/+12spxTXjhw5oqefflrf+ta3Yj0V4LL+/ve/q6+vT2lpaSHr09LS1NHREaNZAZHh9/tVXl6uz33uc7rpppui/vwf+2BZvXq1LBbLJZe33norOP473/mOXn/9df3xj39UYmKiXC6X+Fbt8sI9z5J06tQpzZ8/X1/96lf1zW9+M0YzH3mGcq4B4HKWL1+uN998UzU1NTF5/qti8qwGuf/++3XPPfdcckxeXl7wn8eOHauxY8fq+uuv16c//Wk5HA7t27dPTqfzCs90ZAv3PJ8+fVq33367ZsyYoWefffYKzy6+hHuuETljx45VYmKiOjs7Q9Z3dnYqPT09RrMChm/FihXasWOH/vznPysrKysmc/jYB8u4ceM0bty4IW3r9/slST6fL5JTikvhnOdTp07p9ttvV2FhoZ577jklJHzsPwgMy3Be0xiepKQkFRYWas+ePcELQP1+v/bs2aMVK1bEdnLAEAQCAd13332qra1VfX29cnNzYzaXj32wDFZTU5P279+vmTNnasyYMTp69KgefvhhTZw4kU9XIujUqVO67bbbdO2112rjxo06e/Zs8DH+DzXy2tra9I9//ENtbW3q6+tTS0uLJCk/P18pKSmxndwItXLlSi1ZskS33HKLbr31Vj355JPq6enRN77xjVhPLa6cP39eR44cCf58/PhxtbS0KDU1VdnZ2TGcWXxZvny5qqur9bvf/U6jR48OXotlt9s1atSo6E4mgEF54403ArfffnsgNTU1YLVaAzk5OYGlS5cGTp48GeupxZXnnnsuIKnfBZG3ZMmSfs/13r17Yz21Ee3pp58OZGdnB5KSkgK33nprYN++fbGeUtzZu3dvv6/dJUuWxHpqcWWg9+Pnnnsu6nPhPiwAAMB4XBwAAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAw3v8H6CrmWZ8R9NoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2_1_c neural network model\n",
    "\n",
    "Layer1 = Dense_d(1,10,0.6)\n",
    "Act1 = ReLU()\n",
    "\n",
    "Layer2 = Dense_d(10,5,0.6)\n",
    "Act2 = ReLU()\n",
    "\n",
    "Layer3 = Dense(5,1)\n",
    "Act3 = Linear()\n",
    "\n",
    "Loss = Mean_Square_Error_loss()\n",
    "\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    Layer1.forward(x_train,0.6)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output,0.6)\n",
    "    Act2.forward(Layer2.output)\n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss = Loss.forward(Act3.output,y_train)\n",
    "    \n",
    "    print(\"Epoch\", epoch,\":\")\n",
    "    print(\"Loss\", loss)    \n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act3.output,y_train)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    Act2.backward(Layer3.b_output)\n",
    "    \n",
    "    Layer2.backward(Act2.b_output,0.6)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,0.6)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)\n",
    "    \n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p,0.6)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output,0.6)\n",
    "Act2.forward(Layer2.output)\n",
    "Layer3.forward(Act2.output)\n",
    "Act3.forward(Layer3.output)\n",
    "a = Act3.output\n",
    "\n",
    "plt.scatter(p,a)\n",
    "\n",
    "loss = Loss.forward(Act3.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d234318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "Loss [9.07338473-0.06809937j]\n",
      "--------------------------\n",
      "Epoch 1 :\n",
      "Loss [1.22287868-0.25596956j]\n",
      "--------------------------\n",
      "Epoch 2 :\n",
      "Loss [1.25571781-0.25858846j]\n",
      "--------------------------\n",
      "Epoch 3 :\n",
      "Loss [1.19617721-0.25851235j]\n",
      "--------------------------\n",
      "Epoch 4 :\n",
      "Loss [1.19503681-0.25837772j]\n",
      "--------------------------\n",
      "Epoch 5 :\n",
      "Loss [1.18625521-0.25831763j]\n",
      "--------------------------\n",
      "Epoch 6 :\n",
      "Loss [1.18628933-0.25831565j]\n",
      "--------------------------\n",
      "Epoch 7 :\n",
      "Loss [1.18554299-0.25830402j]\n",
      "--------------------------\n",
      "Epoch 8 :\n",
      "Loss [1.18537276-0.25829932j]\n",
      "--------------------------\n",
      "Epoch 9 :\n",
      "Loss [1.18523313-0.2582957j]\n",
      "--------------------------\n",
      "Epoch 10 :\n",
      "Loss [1.18518169-0.25829409j]\n",
      "--------------------------\n",
      "Epoch 11 :\n",
      "Loss [1.18514945-0.25829307j]\n",
      "--------------------------\n",
      "Epoch 12 :\n",
      "Loss [1.18514905-0.25829305j]\n",
      "--------------------------\n",
      "Epoch 13 :\n",
      "Loss [1.1851451-0.25829289j]\n",
      "--------------------------\n",
      "Epoch 14 :\n",
      "Loss [1.18514463-0.25829288j]\n",
      "--------------------------\n",
      "Epoch 15 :\n",
      "Loss [1.18514446-0.25829287j]\n",
      "--------------------------\n",
      "Epoch 16 :\n",
      "Loss [1.18514436-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 17 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 18 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Epoch 19 :\n",
      "Loss [1.18514434-0.25829286j]\n",
      "--------------------------\n",
      "Loss for testing dataset: [0.3705645-0.03160435j]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2_1_d neural network model\n",
    "\n",
    "Layer1 = Dense_d(1,10,0.6)\n",
    "Act1 = Sigmoid()\n",
    "\n",
    "Layer2 = Dense_d(10,5,0.6)\n",
    "Act2 = Sigmoid()\n",
    "\n",
    "Layer3 = Dense(5,1)\n",
    "Act3 = Linear()\n",
    "\n",
    "Loss = Mean_Square_Error_loss()\n",
    "\n",
    "Optimizer = SGD(0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    #forward\n",
    "    Layer1.forward(x_train,0.6)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output,0.6)\n",
    "    Act2.forward(Layer2.output)\n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss = Loss.forward(Act3.output,y_train)\n",
    "    \n",
    "    print(\"Epoch\", epoch,\":\")\n",
    "    print(\"Loss\", loss)    \n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act3.output,y_train)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    Act2.backward(Layer3.b_output)\n",
    "    \n",
    "    Layer2.backward(Act2.b_output,0.6)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,0.6)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)\n",
    "    \n",
    "#Testing Step:\n",
    "p = x_test\n",
    "Layer1.forward(p,0.6)\n",
    "Act1.forward(Layer1.output)\n",
    "Layer2.forward(Act1.output,0.6)\n",
    "Act2.forward(Layer2.output)\n",
    "Layer3.forward(Act2.output)\n",
    "Act3.forward(Layer3.output)\n",
    "a = Act3.output\n",
    "\n",
    "plt.scatter(p,a)\n",
    "\n",
    "loss = Loss.forward(Act3.output,y_test)\n",
    "print(\"Loss for testing dataset:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b6748",
   "metadata": {},
   "source": [
    "# 4-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e09d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f8ccf73",
   "metadata": {},
   "source": [
    "# 4-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6758ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        neuron_output = np.exp(inputs-np.max(inputs,keepdims=True))\n",
    "        self.output = neuron_output/np.sum(neuron_output,keepdims=True)\n",
    "    \n",
    "    def backward(self,b_input):\n",
    "        self.b_output = b_input\n",
    "        for i , (item1 , item2) in enumerate(zip(self.output,b_input)):\n",
    "            item1 = item1.reshape(-1,1)\n",
    "            #sd means softmax derivative\n",
    "            sd = np.diagflat(item1)-np.dot(item1,item1.T)\n",
    "            #if j=k: S_j-(S_j)^2\n",
    "            #if j!=k: 0-(S_j)*(S_k)\n",
    "            self.b_output[i] = np.dot(sd,item2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa7e7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain=pd.read_csv(\"mnist_train.csv\",delimiter=\",\", dtype=str)\n",
    "datatest=pd.read_csv(\"mnist_test.csv\",delimiter=\",\", dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "649dccee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['5', '0', '0', ..., '0', '0', '0'],\n",
       "       ['0', '0', '0', ..., '0', '0', '0'],\n",
       "       ['4', '0', '0', ..., '0', '0', '0'],\n",
       "       ...,\n",
       "       ['5', '0', '0', ..., '0', '0', '0'],\n",
       "       ['6', '0', '0', ..., '0', '0', '0'],\n",
       "       ['8', '0', '0', ..., '0', '0', '0']], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain=np.array(datatrain)\n",
    "datatest=np.array(datatrain)\n",
    "datatest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c52b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = datatrain[0:55000].T[1:].T\n",
    "t = datatrain[0:55000].T[0].reshape(-1,1)\n",
    "\n",
    "pv = datatrain[0:5000].T[1:].T\n",
    "tv = datatrain[0:5000].T[0].reshape(-1,1)\n",
    "\n",
    "p_test = datatest[0:500].T[1:].T\n",
    "t_test = datatest[0:500].T[0].reshape(-1,1)\n",
    "\n",
    "x_train=p\n",
    "y_train=t\n",
    "\n",
    "x_valid=pv\n",
    "y_valid=tv\n",
    "\n",
    "x_test=p_test\n",
    "y_test=t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fe284e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)\n",
    "x_test = x_test.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "x_train = x_train.astype(int)\n",
    "x_valid =  x_valid.astype(int)\n",
    "y_valid = y_valid.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94781e01",
   "metadata": {},
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c345cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical_cross_entroy_loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,softmax_output,class_label):\n",
    "        softmax_output = np.clip(softmax_output,0.000001,0.999999)\n",
    "        cc = np.sum(softmax_output*class_label,axis=1)\n",
    "        return np.sum(-np.log(cc)).reshape(-1)\n",
    "\n",
    "    \n",
    "    def backward(self,softmax_output,class_label):\n",
    "        softmax_output = np.clip(softmax_output,0.000001,0.999999)\n",
    "        self.b_output = -class_label/softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba3c4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self,learning_rate = 0.001,momentum=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "    def update(self,layer):\n",
    "        if self.momentum:\n",
    "            weight_update = self.momentum*layer.weight_history\\\n",
    "            + (1-self.momentum)*(-self.learning_rate*layer.g_w)\n",
    "            layer.weight_update = weight_update\n",
    "            bias_update = self.momentum*layer.weight_history\\\n",
    "            + (1-self.momentum)*(-self.learning_rate*layer.g_b)\n",
    "            layer.bias_update = bias_update\n",
    "        else:\n",
    "            weight_update = - self.learning_rate*layer.g_w\n",
    "            bias_update = - self.learning_rate*layer.g_b\n",
    "        layer.w = layer.w + weight_update\n",
    "        layer.b = layer.b + bias_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b44d9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1 = Dense(784,10)\n",
    "Act1 = Softmax()\n",
    "Loss = Categorical_cross_entroy_loss()\n",
    "Optimizer = SGD(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "205f2485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: \n",
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09879984429752066\n",
      "Validation data: \n",
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09581032\n",
      "--------------------------\n",
      "Train data: \n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09879984429752066\n",
      "Validation data: \n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09581032\n",
      "--------------------------\n",
      "Train data: \n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09879984429752066\n",
      "Validation data: \n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09581032\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for epoch in range(100):\n",
    "    \n",
    "    #forward\n",
    "    Layer1.forward(x_train)\n",
    "    Act1.forward(Layer1.output)\n",
    "    loss_train = Loss.forward(Act1.output,y_train)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict_train = np.argmax(Act1.output,axis = 1)\n",
    "    accuracy_train = np.mean(y_train == y_predict_train)\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act1.output,y_train)\n",
    "    Act1.backward(Loss.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    \n",
    "        \n",
    "    if epoch==0: \n",
    "        pass\n",
    "    else:\n",
    "        if accuracy_valid >= pre_acc_valid: \n",
    "            if c<2:\n",
    "                c+=1\n",
    "            else:\n",
    "                break \n",
    "        else:\n",
    "            c=0\n",
    "        \n",
    "    \n",
    "    Layer1.forward(x_valid)\n",
    "    Act1.forward(Layer1.output)\n",
    "    loss_valid = Loss.forward(Act1.output,y_valid)\n",
    "    # Report\n",
    "    y_predict_valid = np.argmax(Act1.output,axis = 1)\n",
    "    accuracy_valid = np.mean(y_valid == y_predict_valid)\n",
    "    #for i in range(len(accuracy_valid)):\n",
    "        #accuracy_valid[i]=round(accuracy_valid[i],12)\n",
    "    \n",
    "    print('Train data: ')\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss_train}')\n",
    "    print(f'Accuracy: {accuracy_train}')\n",
    "    \n",
    "    print('Validation data: ')\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss_valid}')\n",
    "    print(f'Accuracy: {accuracy_valid}')\n",
    "    print('--------------------------')\n",
    "\n",
    "    #backward\n",
    "    Loss.backward(Act1.output,y_valid)\n",
    "    Act1.backward(Loss.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    pre_acc_valid=accuracy_valid\n",
    "    pre_loss=loss_valid\n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd74abcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:4\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:5\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:6\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:7\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:8\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:9\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:10\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:11\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:12\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:13\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:14\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:15\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:16\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:17\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:18\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:19\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:20\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:21\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:22\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:23\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:24\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:25\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:26\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:27\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:28\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:29\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:30\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:31\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:32\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:33\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:34\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:35\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:36\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:37\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:38\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:39\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:40\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:41\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:42\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:43\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:44\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:45\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:46\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:47\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:48\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:49\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:50\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:51\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:52\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:53\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:54\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:55\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:56\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:57\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:58\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:59\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:60\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:61\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:62\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:63\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:64\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:65\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:66\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:67\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:68\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:69\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:70\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:71\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:72\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:73\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:74\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:75\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:76\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:77\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:78\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:79\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:80\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:81\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:82\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:83\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:84\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:85\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:86\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:87\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:88\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:89\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:90\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:91\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:92\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:93\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:94\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:95\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:96\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:97\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:98\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n",
      "Epoch:99\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100264\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_test)\n",
    "    Act1.forward(Layer1.output)\n",
    "    loss_test = Loss.forward(Act1.output,y_test)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict_test = np.argmax(Act1.output,axis = 1)\n",
    "    accuracy_test = np.mean(y_test == y_predict_test)\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss_test}')\n",
    "    print(f'Accuracy: {accuracy_test}')\n",
    "    print('--------------------------')\n",
    "    #backward\n",
    "    Loss.backward(Act1.output,y_test)\n",
    "    Act1.backward(Loss.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    Optimizer.update(Layer1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "316cb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62161411",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1 = Dense(784,30)\n",
    "Act1=ReLU()\n",
    "Layer2 = Dense(30,10)\n",
    "\n",
    "Act2= Softmax()\n",
    "Loss = Categorical_cross_entroy_loss()\n",
    "Optimizer = SGD(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7859f1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: \n",
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09874610909090908\n",
      "Test data: \n",
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09574252\n",
      "--------------------------\n",
      "Train data: \n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09874610909090908\n",
      "Test data: \n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09574252\n",
      "--------------------------\n",
      "Train data: \n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09874610909090908\n",
      "Test data: \n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09574252\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_train)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss_train = Loss.forward(Act2.output,y_train)\n",
    "    \n",
    "    y_predict_train = np.argmax(Act1.output,axis = 1)\n",
    "    accuracy_train = np.mean(y_train == y_predict_train)\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_train)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    Layer1.forward(x_valid)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss_valid = Loss.forward(Act2.output,y_valid)\n",
    "    \n",
    "    y_predict_valid = np.argmax(Act1.output,axis = 1)\n",
    "    accuracy_valid = np.mean(y_valid == y_predict_valid)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #for i in range(len(accuracy_valid)):\n",
    "       # accuracy_valid[i]=round(accuracy_valid[i],12)\n",
    "        \n",
    "    if epoch==0: \n",
    "        pass\n",
    "    else:\n",
    "        if accuracy_valid <= pre_acc_valid : \n",
    "            if c<2:\n",
    "                c+=1\n",
    "            else:\n",
    "                break \n",
    "        else:\n",
    "            c=0\n",
    "    print('Train data: ')\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss_train}')\n",
    "    print(f'Accuracy: {accuracy_train}')\n",
    "    \n",
    "    print('Test data: ')\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss_valid}')\n",
    "    print(f'Accuracy: {accuracy_valid}')\n",
    "    print('--------------------------')\n",
    "\n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_valid)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    pre_acc_valid=accuracy_valid\n",
    "    pre_loss=loss_valid\n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8e858fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:4\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:5\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:6\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:7\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:8\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:9\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:10\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:11\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:12\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:13\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:14\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:15\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:16\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:17\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:18\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:19\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:20\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:21\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:22\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:23\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:24\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:25\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:26\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:27\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:28\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:29\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:30\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:31\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:32\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:33\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:34\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:35\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:36\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:37\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:38\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:39\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:40\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:41\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:42\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:43\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:44\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:45\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:46\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:47\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:48\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:49\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:50\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:51\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:52\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:53\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:54\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:55\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:56\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:57\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:58\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:59\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:60\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:61\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:62\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:63\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:64\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:65\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:66\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:67\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:68\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:69\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:70\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:71\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:72\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:73\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:74\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:75\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:76\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:77\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:78\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:79\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:80\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:81\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:82\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:83\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:84\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:85\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:86\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:87\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:88\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:89\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:90\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:91\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:92\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:93\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:94\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:95\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:96\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:97\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:98\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:99\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "\n",
    "    #forward\n",
    "    Layer1.forward(x_test)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_test)\n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_test == y_predict)\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('--------------------------')\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cf455a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "667c51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1 = Dense(784,30)\n",
    "Act1=ReLU()\n",
    "Layer2 = Dense(30,10)\n",
    "Act2= Softmax()\n",
    "\n",
    "Layer3 = Dense(10,10)\n",
    "Act3= Softmax()\n",
    "\n",
    "Loss = Categorical_cross_entroy_loss()\n",
    "Optimizer = SGD(0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cd242f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.0987999788429752\n",
      "Validation Data\n",
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.10999296\n",
      "--------------------------\n",
      "Train Data\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09880004099173553\n",
      "Validation Data\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09580072\n",
      "--------------------------\n",
      "Train Data\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09880004099173553\n",
      "Validation Data\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09580072\n",
      "--------------------------\n",
      "Train Data\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09880004099173553\n",
      "Validation Data\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09580072\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_train)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    \n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss_train = Loss.forward(Act3.output,y_train)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict_train = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy_train = np.mean(y_train == y_predict_train)\n",
    "   \n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act3.output,y_train)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    \n",
    "    Act2.backward(Layer3.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    \n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    \n",
    "    Layer1.forward(x_valid)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    \n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss_valid = Loss.forward(Act3.output,y_valid)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict_valid = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy_valid = np.mean(y_valid == y_predict_valid)\n",
    "    \n",
    "    print('Train Data')\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss_train}')\n",
    "    print(f'Accuracy: {accuracy_train}')\n",
    "    print('Validation Data')\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss_valid}')\n",
    "    print(f'Accuracy: {accuracy_valid}')\n",
    "    print('--------------------------')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #for i in range(len(accuracy_valid)):\n",
    "        #accuracy_valid[i]=round(accuracy_valid[i],12)\n",
    "        \n",
    "    if epoch==0: \n",
    "        pass\n",
    "    else:\n",
    "        if accuracy_valid <= pre_acc_valid : \n",
    "            if c<2:\n",
    "                c+=1\n",
    "            else:\n",
    "                break \n",
    "        else:\n",
    "            c=0\n",
    "            \n",
    "    #backward\n",
    "    Loss.backward(Act3.output,y_valid)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    \n",
    "    Act2.backward(Layer3.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    \n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    pre_acc_valid=accuracy_valid\n",
    "    pre_loss=loss_valid\n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e3f6e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.104\n",
      "--------------------------\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100024\n",
      "--------------------------\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100016\n",
      "--------------------------\n",
      "Epoch:4\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:5\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:6\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:7\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:8\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:9\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:10\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:11\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:12\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:13\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:14\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:15\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:16\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:17\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:18\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:19\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:20\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:21\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:22\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:23\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:24\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:25\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:26\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:27\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:28\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:29\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:30\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:31\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:32\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:33\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:34\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:35\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:36\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:37\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:38\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:39\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:40\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:41\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:42\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:43\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:44\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:45\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:46\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:47\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:48\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:49\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:50\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:51\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:52\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:53\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:54\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:55\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:56\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:57\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:58\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:59\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:60\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:61\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:62\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:63\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:64\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:65\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:66\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:67\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:68\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:69\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:70\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:71\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:72\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:73\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:74\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:75\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:76\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:77\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:78\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:79\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:80\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:81\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:82\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:83\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:84\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:85\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:86\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:87\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:88\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:89\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:90\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:91\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:92\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:93\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:94\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:95\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:96\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:97\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:98\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n",
      "Epoch:99\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100008\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_test)\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    \n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss = Loss.forward(Act3.output,y_test)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_test == y_predict)\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('--------------------------')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9956fc36",
   "metadata": {},
   "source": [
    "# 4-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76752a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = datatrain[0:500].T[1:].T\n",
    "t = datatrain[0:500].T[0].reshape(-1,1)\n",
    "p_test = datatest[0:500].T[1:].T\n",
    "t_test = datatest[0:500].T[0].reshape(-1,1)\n",
    "x_train=p\n",
    "y_train=t\n",
    "x_test=p_test\n",
    "y_test=t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de36b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)\n",
    "x_test = x_test.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "x_train = x_train.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d71399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1 = Dense(784,10)\n",
    "Act1 = Softmax()\n",
    "Loss = Categorical_cross_entroy_loss()\n",
    "Optimizer = SGD(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b886384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:4\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:5\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:6\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:7\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:8\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:9\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:10\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:11\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:12\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:13\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:14\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:15\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:16\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:17\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:18\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:19\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:20\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:21\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:22\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:23\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:24\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:25\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:26\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:27\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:28\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:29\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:30\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:31\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:32\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:33\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:34\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:35\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:36\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:37\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:38\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:39\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:40\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:41\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:42\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:43\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:44\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:45\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:46\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:47\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:48\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:49\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:50\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:51\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:52\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:53\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:54\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:55\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:56\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:57\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:58\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:59\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:60\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:61\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:62\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:63\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:64\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:65\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:66\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:67\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:68\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:69\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:70\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:71\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:72\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:73\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:74\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:75\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:76\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:77\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:78\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:79\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:80\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:81\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:82\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:83\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:84\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:85\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:86\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:87\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:88\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:89\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:90\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:91\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:92\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:93\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:94\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:95\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:96\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:97\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:98\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:99\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_train)\n",
    "    Act1.forward(Layer1.output)\n",
    "    loss = Loss.forward(Act1.output,y_train)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act1.output,axis = 1)\n",
    "    accuracy = np.mean(y_train == y_predict)\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act1.output,y_train)\n",
    "    Act1.backward(Loss.b_output)\n",
    "    Layer1.backward(Act1.b_output)\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3248bb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:4\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:5\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:6\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:7\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:8\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:9\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:10\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:11\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:12\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:13\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:14\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:15\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:16\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:17\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:18\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:19\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:20\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:21\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:22\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:23\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:24\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:25\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:26\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:27\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:28\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:29\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:30\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:31\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:32\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:33\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:34\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:35\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:36\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:37\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:38\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:39\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:40\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:41\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:42\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:43\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:44\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:45\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:46\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:47\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:48\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:49\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:50\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:51\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:52\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:53\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:54\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:55\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:56\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:57\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:58\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:59\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:60\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:61\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:62\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:63\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:64\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:65\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:66\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:67\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:68\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:69\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:70\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:71\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:72\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:73\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:74\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:75\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:76\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:77\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:78\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:79\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:80\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:81\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:82\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:83\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:84\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:85\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:86\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:87\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:88\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:89\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:90\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:91\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:92\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:93\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:94\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:95\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:96\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:97\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:98\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:99\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_test)\n",
    "    Act1.forward(Layer1.output)\n",
    "    loss = Loss.forward(Act1.output,y_test)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act1.output,axis = 1)\n",
    "    accuracy = np.mean(y_test == y_predict)\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('--------------------------')\n",
    "    \n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60dda6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1 = Dense_d(784,30,0.2)\n",
    "Act1=ReLU()\n",
    "Layer2 = Dense(30,10)\n",
    "\n",
    "Act2= Softmax()\n",
    "Loss = Categorical_cross_entroy_loss()\n",
    "Optimizer = SGD(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e8aed16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099876\n",
      "--------------------------\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09992\n",
      "--------------------------\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.10002\n",
      "--------------------------\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100028\n",
      "--------------------------\n",
      "Epoch:4\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09992\n",
      "--------------------------\n",
      "Epoch:5\n",
      "Loss: [inf]\n",
      "Accuracy: 0.10006\n",
      "--------------------------\n",
      "Epoch:6\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100128\n",
      "--------------------------\n",
      "Epoch:7\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100868\n",
      "--------------------------\n",
      "Epoch:8\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099852\n",
      "--------------------------\n",
      "Epoch:9\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099776\n",
      "--------------------------\n",
      "Epoch:10\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099928\n",
      "--------------------------\n",
      "Epoch:11\n",
      "Loss: [inf]\n",
      "Accuracy: 0.10148\n",
      "--------------------------\n",
      "Epoch:12\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09942\n",
      "--------------------------\n",
      "Epoch:13\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:14\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:15\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:16\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:17\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:18\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:19\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:20\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:21\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:22\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:23\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:24\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:25\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099472\n",
      "--------------------------\n",
      "Epoch:26\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099824\n",
      "--------------------------\n",
      "Epoch:27\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:28\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:29\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:30\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:31\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:32\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:33\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:34\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:35\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:36\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:37\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:38\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:39\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:40\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:41\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099824\n",
      "--------------------------\n",
      "Epoch:42\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:43\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:44\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:45\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:46\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:47\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:48\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:49\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:50\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:51\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:52\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:53\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:54\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:55\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099824\n",
      "--------------------------\n",
      "Epoch:56\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:57\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:58\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:59\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:60\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:61\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:62\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:63\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:64\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:65\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:66\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:67\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:68\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:69\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099912\n",
      "--------------------------\n",
      "Epoch:70\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:71\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:72\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:73\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:74\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:75\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:76\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:77\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:78\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:79\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:80\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:81\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:82\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:83\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:84\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099604\n",
      "--------------------------\n",
      "Epoch:85\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:86\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:87\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:88\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:89\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:90\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:91\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:92\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:93\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:94\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:95\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:96\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:97\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:98\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:99\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_train,'train')\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_train)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_train == y_predict)\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act2.output,y_train)\n",
    "    Act2.backward(Loss.b_output)\n",
    "    Layer2.backward(Act2.b_output)\n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,'train')\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a429a5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:4\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:5\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:6\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:7\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:8\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:9\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:10\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:11\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:12\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:13\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:14\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:15\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:16\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:17\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:18\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:19\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:20\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:21\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:22\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:23\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:24\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:25\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:26\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:27\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:28\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:29\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:30\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:31\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:32\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:33\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:34\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:35\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:36\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:37\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:38\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:39\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:40\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:41\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:42\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:43\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:44\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:45\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:46\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:47\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:48\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:49\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:50\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:51\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:52\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:53\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:54\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:55\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:56\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:57\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:58\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:59\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:60\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:61\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:62\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:63\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:64\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:65\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:66\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:67\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:68\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:69\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:70\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:71\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:72\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:73\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:74\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:75\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:76\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:77\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:78\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:79\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:80\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:81\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:82\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:83\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:84\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:85\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:86\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:87\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:88\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:89\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:90\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:91\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:92\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:93\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:94\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:95\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:96\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:97\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:98\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:99\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_test,'test')\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output)\n",
    "    Act2.forward(Layer2.output)\n",
    "    loss = Loss.forward(Act2.output,y_test)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_test == y_predict)\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('--------------------------')\n",
    "\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab49d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1 = Dense_d(784,30,0.2)\n",
    "Act1=ReLU()\n",
    "Layer2 = Dense_d(30,10,0.2)\n",
    "Act2= Softmax()\n",
    "\n",
    "Layer3 = Dense(10,10)\n",
    "Act3= Softmax()\n",
    "\n",
    "Loss = Categorical_cross_entroy_loss()\n",
    "Optimizer = SGD(0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3f85d8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099976\n",
      "--------------------------\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100084\n",
      "--------------------------\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100232\n",
      "--------------------------\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.10004\n",
      "--------------------------\n",
      "Epoch:4\n",
      "Loss: [inf]\n",
      "Accuracy: 0.10086\n",
      "--------------------------\n",
      "Epoch:5\n",
      "Loss: [inf]\n",
      "Accuracy: 0.100444\n",
      "--------------------------\n",
      "Epoch:6\n",
      "Loss: [inf]\n",
      "Accuracy: 0.10234\n",
      "--------------------------\n",
      "Epoch:7\n",
      "Loss: [inf]\n",
      "Accuracy: 0.1\n",
      "--------------------------\n",
      "Epoch:8\n",
      "Loss: [inf]\n",
      "Accuracy: 0.08284\n",
      "--------------------------\n",
      "Epoch:9\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:10\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:11\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099912\n",
      "--------------------------\n",
      "Epoch:12\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099868\n",
      "--------------------------\n",
      "Epoch:13\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099516\n",
      "--------------------------\n",
      "Epoch:14\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09978\n",
      "--------------------------\n",
      "Epoch:15\n",
      "Loss: [inf]\n",
      "Accuracy: 0.094104\n",
      "--------------------------\n",
      "Epoch:16\n",
      "Loss: [inf]\n",
      "Accuracy: 0.080912\n",
      "--------------------------\n",
      "Epoch:17\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:18\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:19\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:20\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:21\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:22\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:23\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099912\n",
      "--------------------------\n",
      "Epoch:24\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:25\n",
      "Loss: [inf]\n",
      "Accuracy: 0.104\n",
      "--------------------------\n",
      "Epoch:26\n",
      "Loss: [inf]\n",
      "Accuracy: 0.080288\n",
      "--------------------------\n",
      "Epoch:27\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:28\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099912\n",
      "--------------------------\n",
      "Epoch:29\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099824\n",
      "--------------------------\n",
      "Epoch:30\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099516\n",
      "--------------------------\n",
      "Epoch:31\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09714\n",
      "--------------------------\n",
      "Epoch:32\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078156\n",
      "--------------------------\n",
      "Epoch:33\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:34\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:35\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:36\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:37\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:38\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:39\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099912\n",
      "--------------------------\n",
      "Epoch:40\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09978\n",
      "--------------------------\n",
      "Epoch:41\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099648\n",
      "--------------------------\n",
      "Epoch:42\n",
      "Loss: [inf]\n",
      "Accuracy: 0.104\n",
      "--------------------------\n",
      "Epoch:43\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:44\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:45\n",
      "Loss: [inf]\n",
      "Accuracy: 0.0879\n",
      "--------------------------\n",
      "Epoch:46\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078044\n",
      "--------------------------\n",
      "Epoch:47\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078364\n",
      "--------------------------\n",
      "Epoch:48\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:49\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:50\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:51\n",
      "Loss: [inf]\n",
      "Accuracy: 0.095424\n",
      "--------------------------\n",
      "Epoch:52\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:53\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:54\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:55\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:56\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:57\n",
      "Loss: [inf]\n",
      "Accuracy: 0.104\n",
      "--------------------------\n",
      "Epoch:58\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:59\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:60\n",
      "Loss: [inf]\n",
      "Accuracy: 0.102232\n",
      "--------------------------\n",
      "Epoch:61\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:62\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:63\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:64\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:65\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:66\n",
      "Loss: [inf]\n",
      "Accuracy: 0.104\n",
      "--------------------------\n",
      "Epoch:67\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:68\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:69\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:70\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:71\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:72\n",
      "Loss: [inf]\n",
      "Accuracy: 0.087776\n",
      "--------------------------\n",
      "Epoch:73\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:74\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:75\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:76\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:77\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099912\n",
      "--------------------------\n",
      "Epoch:78\n",
      "Loss: [inf]\n",
      "Accuracy: 0.086756\n",
      "--------------------------\n",
      "Epoch:79\n",
      "Loss: [inf]\n",
      "Accuracy: 0.084072\n",
      "--------------------------\n",
      "Epoch:80\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078052\n",
      "--------------------------\n",
      "Epoch:81\n",
      "Loss: [inf]\n",
      "Accuracy: 0.081224\n",
      "--------------------------\n",
      "Epoch:82\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:83\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099516\n",
      "--------------------------\n",
      "Epoch:84\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099296\n",
      "--------------------------\n",
      "Epoch:85\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09692\n",
      "--------------------------\n",
      "Epoch:86\n",
      "Loss: [inf]\n",
      "Accuracy: 0.104\n",
      "--------------------------\n",
      "Epoch:87\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:88\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:89\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:90\n",
      "Loss: [inf]\n",
      "Accuracy: 0.09956\n",
      "--------------------------\n",
      "Epoch:91\n",
      "Loss: [inf]\n",
      "Accuracy: 0.098944\n",
      "--------------------------\n",
      "Epoch:92\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:93\n",
      "Loss: [inf]\n",
      "Accuracy: 0.078\n",
      "--------------------------\n",
      "Epoch:94\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:95\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:96\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:97\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:98\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:99\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_train,'train')\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output,'train')\n",
    "    Act2.forward(Layer2.output)\n",
    "    \n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss = Loss.forward(Act3.output,y_train)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_train == y_predict)\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('--------------------------')\n",
    "    \n",
    "    #backward\n",
    "    Loss.backward(Act3.output,y_train)\n",
    "    Act3.backward(Loss.b_output)\n",
    "    Layer3.backward(Act3.b_output)\n",
    "    \n",
    "    Act2.backward(Layer3.b_output)\n",
    "    Layer2.backward(Act2.b_output,'train')\n",
    "    \n",
    "    Act1.backward(Layer2.b_output)\n",
    "    Layer1.backward(Act1.b_output,'train')\n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3e18293",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:1\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:2\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:3\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:4\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:5\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:6\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:7\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:8\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:9\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:10\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansar9811291\\AppData\\Local\\Temp\\ipykernel_4508\\1580103707.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(-np.log(cc)).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:12\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:13\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:14\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:15\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:16\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:17\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:18\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:19\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:20\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:21\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:22\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:23\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:24\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:25\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:26\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:27\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:28\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:29\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:30\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:31\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:32\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:33\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:34\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:35\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:36\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:37\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:38\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:39\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:40\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:41\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:42\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:43\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:44\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:45\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:46\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:47\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:48\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:49\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:50\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:51\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:52\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:53\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:54\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:55\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:56\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:57\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:58\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:59\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:60\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:61\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:62\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:63\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:64\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:65\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:66\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:67\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:68\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:69\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:70\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:71\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:72\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:73\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:74\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:75\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:76\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:77\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:78\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:79\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:80\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:81\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:82\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:83\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:84\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:85\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:86\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:87\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:88\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:89\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:90\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:91\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:92\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:93\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:94\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:95\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:96\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:97\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:98\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n",
      "Epoch:99\n",
      "Loss: [inf]\n",
      "Accuracy: 0.099956\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    #forward\n",
    "    Layer1.forward(x_test,'test')\n",
    "    Act1.forward(Layer1.output)\n",
    "    Layer2.forward(Act1.output,'test')\n",
    "    Act2.forward(Layer2.output)\n",
    "    \n",
    "    Layer3.forward(Act2.output)\n",
    "    Act3.forward(Layer3.output)\n",
    "    loss = Loss.forward(Act3.output,y_test)\n",
    "    \n",
    "    \n",
    "    # Report\n",
    "    y_predict = np.argmax(Act2.output,axis = 1)\n",
    "    accuracy = np.mean(y_test == y_predict)\n",
    "    print(f'Epoch:{epoch}')\n",
    "    print(f'Loss: {loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('--------------------------')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #update params\n",
    "    Optimizer.update(Layer1)\n",
    "    Optimizer.update(Layer2)\n",
    "    Optimizer.update(Layer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d68a0",
   "metadata": {},
   "source": [
    "# 4-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f22769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e079637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb4e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
